\documentclass[reprint, superscriptaddress]{revtex4-1}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{upgreek}
\usepackage[table,usenames,dvipsnames]{xcolor}
%\usepackage{tikz}
%\usetikzlibrary{calc}
\usepackage{hyperref}
\usepackage{setspace}

\setcitestyle{super}

\hypersetup{
  colorlinks,
  linkcolor={blue!80!black},
  citecolor={green!20!black},
  urlcolor={blue!80!black}
}


\definecolor{DarkBlue}{RGB}{0,0,64}
\definecolor{DarkBrown}{RGB}{64,20,10}
\definecolor{DarkGreen}{RGB}{0,64,0}
\definecolor{DarkPurple}{RGB}{128,0,84}
\definecolor{LightGray}{gray}{0.85}
% annotation macros
\newcommand{\repl}[2]{{\color{gray} [#1] }{\color{blue} #2}}
\newcommand{\add}[1]{{\color{blue} #1}}
\newcommand{\del}[1]{{\color{gray} [#1]}}
\newcommand{\note}[1]{{\color{DarkGreen}\footnotesize \textsc{Note.} #1}}

\newcommand{\MI}{\mathcal I} % mutual information

\begin{document}
\title{Bias of the configuration entropy estimated from counting-based methods}

\author{Justin A. Drake}
\affiliation{
Sealy Center for Structural Biology and Molecular Biophysics,
The University of Texas Medical Branch,
Galveston, Texas 77555-0304, USA}
\author{Danielle D. Seckfort}
\affiliation{
Quantitative and Computational Biosciences,
Baylor College of Medicine, Houston, Texas 77030, USA}
\author{Omneya Nassar}
\affiliation{
Sealy Center for Structural Biology and Molecular Biophysics,
The University of Texas Medical Branch,
Galveston, Texas 77555-0304, USA}
\author{B. Montgomery Pettitt}
\email{mpettitt@utmb.edu}
\affiliation{
Sealy Center for Structural Biology and Molecular Biophysics,
The University of Texas Medical Branch,
Galveston, Texas 77555-0304, USA}
\affiliation{
Quantitative and Computational Biosciences,
Baylor College of Medicine, Houston, Texas 77030, USA}

\begin{abstract}
We show that the configuration entropy estimated from methods based on counting occurrences
carries a negative bias
that is inversely proportional to the simulation length.
%
In particular, for the mutual information expansion method,
we show that the magnitude of the bias can grow rapidly with the order of the expansion.
\end{abstract}

\maketitle


\section{Introduction}

Entropy is a key concept of statistical mechanics that provides a quantitative measure of
the number of possible states a system can go through during its evolution.
%
In particular, the configuration entropy is defined,
for $M$ coarse-grained states in the configuration space, as
%
\begin{equation}
  S
  \equiv
  -k_B \sum_{\alpha = 1}^M p(\alpha) \, \ln p(\alpha)
  ,
  \label{eq:entropy_def}
\end{equation}
%
where $p(\alpha)$ is the probability of discrete state $\alpha = 1, \dots, M$,
and $k_B$ is the Boltzmann constant.
%
For the even distribution, $p(\alpha) \equiv 1/M$,
we can readily verify that $S \equiv k_B \, \ln M$
is proportional to the logarithm of the number of states.

We can readily apply Eq.~\eqref{eq:entropy_def} to a trajectory
obtained from a molecular dynamics (MD) or Monte Carlo (MC) simulation,
with the probabilities, $p(\alpha)$'s,
estimated from the observed occurrences of the states.
%
This straightforward application also furnishes the basis
of many more-advanced counting-based methods for estimating the configuration entropy\cite{hnizdo2007, killian2007}.
%
Among them, the method of mutual information expansion (MIE)\cite{killian2007} is notable
for its potential to handle a vast but decomposable state space common in a complex system.
%
The MIE method applies to a many-body system
whose degrees of freedom are only loosely coupled.
%
Starting from the sum of marginal entropies of individual components,
the method offers a cascade of levels of corrections
to systematically absorb the contributions of higher-order correlations
of increasingly larger groups, such as pairs, triplets, quartets, etc.

However, the above counting-based methods
tends to underestimate the configuration entropy
because a realistic simulation of finite length
often cannot fully sample the state space.
%
Thus, it is useful to understand the nature of this bias
and its dependence on the sample size.
%
This knowledge can hopefully help us derive a finite-size correction.

Here we wish to show that the configuration entropy derived
from the counting-based methods (including the MIE method)
generally carries a negative bias
that roughly scales inversely with the trajectory length.
%
Such a bias can be readily corrected to the leading order
by a blocking procedure.
%
In addition, the bias of the MIE method, in particular,
can grow dramatically with the order of tuples.
%
In some cases, we can reduce the magnitude of the bias
by imposing some restrictions on the higher-order tuples included.



\section{Method}

\subsection{\label{sec:fsbias}
Finite-size bias in the direct counting method}

We will first discuss the finite-size bias associated with
the direct application of Eq.~\eqref{eq:entropy_def},
which will be referred to as the direct counting method below.
%
Here, we estimate the probability, $p(\alpha)$,
by the frequency, $\hat{p}(\alpha) = \hat{n}(\alpha) / t$,
where $\hat{n}(\alpha)$ is the number of times state $\alpha$ occurring in the trajectory of $t$ steps,
and the entropy estimate is
%
\begin{equation}
  \hat S
  =
  -k_B \sum_{\alpha = 1}^M \hat{p}(\alpha) \, \ln \hat{p}(\alpha)
  .
  \label{eq:entropy_est}
\end{equation}
%
To investigate the error of Eq.~\eqref{eq:entropy_est},
we will imagine an ensemble of trajectories of replica systems
that differ only by the random forces
and define the bias from the difference
between the ensemble average,
$\bigl\langle \hat S \bigr\rangle$,
and the true value, $S$.
%
By the series expansion of $\hat{p}(\alpha)$ around $p(\alpha)$, we have
%
\begin{align}
  \hat S
  =
  S
  &- k_B \sum_{\alpha = 1}^M
    \bigl[\ln p(\alpha) + 1 \bigr]
    \bigl[ \hat{p}(\alpha) - p(\alpha) \bigr]
  \notag\\
  &- k_B \sum_{\alpha = 1}^M
    \frac{ \bigl[ \hat{p}(\alpha) - p(\alpha) \bigr]^2 } { 2 \, p(\alpha) }
  .
  \label{eq:Shat_series}
\end{align}
%
\note{The series expansion of the function, $h(x) = -x \, \ln x$,
around $x_0$,
%
\begin{align*}
  h(x)
  &\approx h(x_0) + h'(x_0) (x - x_0) + \frac{h''(x_0)}{2} (x - x_0)^2 \\
  &\approx h(x_0) - (\ln x_0 + 1) ( x - x_0) - \frac{1}{2 \, x_0} (x - x_0)^2
  .
\end{align*}
}
%
Assuming the sampling is unbiased,
we have $\langle \hat{p}(\alpha) \rangle = p(\alpha)$.
%
Thus, after ensemble averaging, we get
%
\begin{align}
  \bigl\langle \hat S \bigr\rangle
  \approx
  S - k_B \sum_{\alpha = 1}^M
    \frac{ \operatorname{var} \hat{p}(\alpha) } { 2 \, p(\alpha) }
  =
  S - k_B \sum_{\alpha = 1}^M
    \frac{ \operatorname{var} \hat{n}(\alpha) } { 2 \, p(\alpha) \, t^2 }
  .
  \label{eq:entest_2nd}
\end{align}

By definition, we have
%
\begin{equation*}
  \hat{n}(\alpha) = \sum_{t' = 1}^t \delta_{\alpha, \alpha(t')},
\end{equation*}
%
where $\alpha(t')$ denotes the state at step $t'$,
and $\delta_{\alpha, \gamma}$ is the Kronecker delta,
which takes the value of $1$ if $\alpha = \gamma$, or $0$ otherwise.
%
We can readily show that
\begin{align*}
  \bigl\langle \hat{n}(\alpha) \bigr\rangle
  &=
  t \, p(\alpha), \\
  %
  \operatorname{var}{\hat{n}(\alpha)}
  &=
  %\left\langle \hat{n}(\alpha)^2 \right\rangle - \langle \hat{n}(\alpha) \rangle^2
  %\\
  %&=
  t \, p(\alpha) \, \bigl( 1 - p(\alpha) \bigr) \, \bigl(2 \, \tau_\alpha + 1\bigr)
  ,
\end{align*}
%
where $\tau_\alpha$ is the integral autocorrelation time of $\delta_{\alpha, \alpha(t)}$.\footnote{
The autocorrelation function is defined as
$$
\kappa_\alpha(t) = \bigl(\delta_{\alpha, \alpha(t)} - p(\alpha)\bigr)
\bigl(\delta_{\alpha, \alpha(0)} - p(\alpha)\bigr),
$$
and the autocorrelation time is given by
$\tau_\alpha = \sum_{t = 1}^\infty \kappa_\alpha(t)/\kappa_\alpha(0)$.
}
%
Thus, we can rewrite Eq.~\eqref{eq:entest_2nd} as
%
\begin{align}
  \bigl\langle \hat S \bigr\rangle
  &\approx
  S - \frac{ a } { t }
  %\notag\\
  %&\approx
  %S - k_B
  %  \frac{ (M - 1) ( 2 \, \tau + 1) } { 2 \, t }
  ,
  \label{eq:Shat_ave}
\end{align}
%
where $a = k_B \sum_{\alpha = 1}^M
    \bigl(1 - p(\alpha)\bigr) ( 2 \, \tau_\alpha + 1)/2$,
%
and $a = (M-1)/2$ for perfect sampling, where $\tau_\alpha \equiv 0$.
%where in the second step,
%we have assumed that all integral autocorrelation times are the same.

This result shows that the entropy estimated
from direct counting
is expected to carry a negative bias,
which is, to the leading order,
inversely proportional to the trajectory length,
or sample size.



\subsection{Corrections to the linear bias}


The above bias can be readily corrected by the following blocking procedure.
%
We will consider the estimate from Eq.~\eqref{eq:entropy_est}
as a function of trajectory length, and denote it as $\hat S(t)$.
%
Then, we can define an estimate, $\hat S(t_b)$,
from a trajectory segment of $t_b$ steps.
%
The ensemble average of $\hat S(t_b)$ also follows
Eq.~\eqref{eq:Shat_ave}, but with $t$ replaced by $t_b$.
%
This allows us to deduce the constants, $S$ and $a$, in Eq.~\eqref{eq:Shat_ave}
by solving the two equations for $\hat S(t)$ and $\hat S(t_b)$.
%
This process is equivalent to a two-point linear regression,
and it results in an extrapolated estimate
%
\begin{align}
    \hat S(t, t_b)
    =
    \frac{ t \, \hat S(t) - t_b \, \hat S(t_b) }
         { t - t_b }
    .
    \label{eq:S_2point}
\end{align}

We can implement Eq.~\eqref{eq:S_2point} in a blocking procedure.
%
We can divide the trajectory into $b$ blocks of equal length,
each of $t_b = t/b$ steps.
%
Then, we can form an estimate $S_{b, (l)}$ for each block $l = 1, \dots, b$ from Eq.~\eqref{eq:entropy_est}.
%
We then use the arithmetic mean of the $b$ block estimates
as the $\hat S(t_b)$ in Eq.~\eqref{eq:S_2point}.
%
The advantage of using the block average instead of
the value from the first block is that
the former make the variance of $\hat S(t, t_b)$
roughly as low as that of $\hat S(t)$,
and thus minimizes the artifact of the extrapolation.


\subsubsection{Higher-order improvement on the correction}

Note that Eq.~\eqref{eq:Shat_ave} is only the first-order expansion
to an infinite series,
%
\begin{equation}
  \hat S = S - \frac{a_1}{t} - \frac{a_2}{t^2} - \cdots
  .
  \label{eq:S_series_invt}
\end{equation}
%
Thus, it is possible, at least in theory,
to further improve the estimate by a higher-order extrapolation
against $1/t$ with more data points.
%
For example, if we can divide the entire trajectory to $c$ blocks
with $c > b$ and obtain the block average of $\hat S(t_c)$
as in the above procedure,
%
The three-point extrapolation estimate reads
$$
\hat S(t, t_b, t_c) =
\frac{ t_b \, \hat S(t, t_b) - t_c \, \hat S(t, t_c) }
     { t_b - t_c },
$$
which can eliminate not only the $1/t$ tail
but also the higher-order $1/t^2$ tail.


\subsubsection{Alternative improvement on the correction}

Another direction to improve Eq.~\eqref{eq:S_2point}
is to fit the function $\hat S(t)$ against a better functional.
%
We note that Eq.~\eqref{eq:Shat_ave} is an asymptotic expansion
that only holds when the frequencies are close to the true probabilities,
and we can expect series expansion, Eq.~\eqref{eq:S_series_invt},
to be convergent only when the effective number of samples,
$t^* = t/(\overline{2 \, \tau_\alpha + 1})$, is greater than
the number of states $M$ (for a large $M$).
%
In the initial stage, $t^* \ll M$,
we would expect the number of states visited to be proportional to $t^*$,
and the entropy estimate, $\hat S(t)$,
to be $\ln t^* = \ln t + \mathrm{const.}$
%
A phenomenological form to accommodate both the initial and asymptotic behavior
would be
$$
  \bigl\langle \hat S \bigr\rangle
  = S - \ln\left(1 + \frac{a}{t}\right).
$$
The two-point extrapolation from this form gives
%
\begin{equation}
  \hat S(t, t_b) = \ln\left(
    \frac{ t - t_b }
         { t \, e^{-\hat S(t)} - t_b \, e^{-\hat S(t_b)} }
  \right)
  .
  \label{eq:S_lnexp_2point}
\end{equation}
%
Equation~\eqref{eq:S_lnexp_2point} is reduced to
Eq.~\eqref{eq:S_2point} in the asymptotic regime,
while it can hopefully improve the result for initial stages.

%As we will see, the same type of bias also exists
%in the more elaborate the MIE method.


\subsection{Kirkwood superposition approximation}


The direct counting method breaks down
for a complex system of many degrees of freedom (DOFs).
%
Because as the number of states increases exponentially with the number of DOFs,
the rapidly diminishing number of samples collected in each state
can readily deteriorate the precision of the entropy estimate.
%
For these many-body systems,
we may alternatively adopt
the mutual information expansion (MIE) method,
which is a more advanced, albeit approximate, method designed for a system
with many largely-independent DOFs.
%
The MIE method is based on the idea of
the Kirkwood superposition approximation (KAS)\cite{kirkwood1935, born1946},
or that of interaction information\cite{mcgill1954},
which we will briefly review below.

We first recall that the (dimensionless) potential of mean force (PMF)
is defined for a distribution, $p(\alpha)$,
as
$$
W(\alpha) = -\ln p(\alpha) = \ln\left[ \frac{1}{p(\alpha)} \right].
$$
The PMF can be interpreted as
the free energy of confining the ensemble to a single state,
or the amount of work
(in the unit of $k_B \, T$'s, with $T$ being the temperature)
required to transfer an average system randomly drawn from the ensemble (of probability $1$)
to state $\alpha$ [of probability $p(\alpha)$].
%
We can now rewrite Eq.~\eqref{eq:entropy_def} as
%
\begin{equation}
  \frac{S}{k_B}
  =
  \sum_{\alpha = 1}^M p(\alpha) \, W(\alpha)
  =
  \overline{ W(\alpha) }
  ,
  \label{eq:S_W}
\end{equation}
%
which shows that the configuration entropy
is the mean PMF.
%
The superposition approximation is based on the intuition
that for a physical or extensive system composed of multiple DOFs,
the PMF of the entire system is roughly decomposable to
the sum of the marginal PMFs of individual DOFs.

Consider a many-body system of $n$ DOFs.
%
We label the DOFs numerically, $1, \dots, n$,
and specify the state as $\alpha = (\alpha_1, \dots, \alpha_n)$.
%
%For simplicity, we will assume that the degrees of freedom are identical.
%
For a subset of DOFs, $I = \{i, j, \dots\}$,
we denote the joint distribution by $p_I(\alpha_I)$,
and define the joint PMF as
%
\begin{equation}
  W_I(\alpha_I) = -\ln p_I(\alpha_I)
  .
  \label{eq:WI_def}
\end{equation}
%
For example,
the marginal PMF of a single DOF $i$ is given by $W_i$ (omitting the argument),
the joint PMF of a pair $\{i, j\}$ by $W_{i, j}$
(dropping the braces ``$\{$'' and ``$\}$'' in the subscript, same below),
that of a triplet $\{i, j, k\}$ by $W_{i, j, k}$, etc.

Now by assuming that the DOFs are largely independent,
we can write the PMF of a pair $\{i, j\}$
as the sum of the marginal PMFs of the two individual DOFs $i$ and $j$
and a hopefully small two-body correction, $\delta W_{i,j}$,
%
\begin{equation}
  W_{i,j}(\alpha_i, \alpha_j)
  =
  W_i(\alpha_i) + W_j(\alpha_j)
  + \delta W_{i,j}(\alpha_i, \alpha_j)
  .
  \label{eq:W_ij}
\end{equation}
%
Similarly, for a triplet, $\{i, j, k\}$,
we can define the three-body correction, $\delta W_{i,j,k}$, via
%
\begin{align}
  &W_{i,j,k}(\alpha_i, \alpha_j, \alpha_k)
  =
  W_i(\alpha_i) + W_j(\alpha_j) + W_k(\alpha_k)
  \notag \\
  &\quad
  + \delta W_{i,j}(\alpha_i, \alpha_j)
  + \delta W_{j,k}(\alpha_j, \alpha_k)
  + \delta W_{k,i}(\alpha_k, \alpha_i)
  \notag \\
  &\quad
  + \delta W_{i,j,k}(\alpha_i, \alpha_j, \alpha_k)
  .
  \label{eq:W_ijk}
\end{align}
%


Generally, we can define the PMF correction, $\delta W_I$,
for an arbitrary set of DOFs, $I$,
implicitly through the recurrence relation,
%
\begin{equation}
  W_I(\alpha_I)
  =
  \sum_{J \subseteq I}
  \delta W_J(\alpha_J)
  ,
  \label{eq:WI_dWJ}
\end{equation}
%
where the sum goes through all subsets $J$'s of $I$,
and we have defined
$\delta W_\emptyset \equiv W_\emptyset \equiv 0$.
%
Note that in this definition,
$\delta W_i \equiv W_i$
for a single DOF $i$.
%
We can readily verify that
Eqs.~\eqref{eq:W_ij} and \eqref{eq:W_ijk}
are special cases of Eq.~\eqref{eq:WI_dWJ}

As Eq.~\eqref{eq:WI_dWJ} is a linear relation,
we can invert it to get an explicit definition of $\delta W_J$
through the inclusion-exclusion principle\cite{bjorklund2007}
% also known as or the inverse M\"{o}bius transform
(cf. Appendix~\ref{sec:inclexcl}),
%
\begin{equation}
  \delta W_J(\alpha_J)
  =
  \sum_{I \subseteq J}
  (-1)^{|J| - |I|}
  W_I(\alpha_I)
  ,
  \label{eq:dWJ_WI}
\end{equation}
%
where $|I|$ denotes the size of subset $I$.
%
Below we will refer to the subset sizes, $|I|$ and $|J|$,
as the orders of $W_I(\alpha_I)$ and $\delta W_J(\alpha_J)$.
%
For example, we have, for a pair, $\{i, j\}$,\footnote{
  Just as we define the PMFs, $W_I$'s, from the joint distributions
  through Eq.~\eqref{eq:WI_def},
  we can associate the corrections, $\delta W_I$'s,
  with the (excess) correlation functions, $g_I$'s, as
  %
  $$
  g_I(\alpha_I) \equiv e^{-\delta W_I(\alpha_I)}
  .
  $$
  %
  This definition is consistent with the usual definition
  of the pair correlation function (as in liquid state theory\cite{hansen}),
  $g_{i,j}(\alpha_i, \alpha_j)$ for a pair, $\{i, j\}$,
  $$
    g_{i,j}(\alpha_i, \alpha_j)
    =
    \frac{ p_{i,j}(\alpha_i, \alpha_j) } { p_i(\alpha_i) \, p_j(\alpha_j) }
    .
  $$
  Similarly, for a triplet, $\{i, j, k\}$, we have
  $$
  g_{i,j,k}(\alpha_i, \alpha_j, \alpha_k)
  =
  \frac{ p_{i,j,k}(\alpha_i, \alpha_j \, \alpha_k) \, p_i(\alpha_i) \, p_j(\alpha_j) \, p_k(\alpha_k) }{ p_{i,j}(\alpha_i, \alpha_j) \, p_{j,k}(\alpha_j, \alpha_k) \, p_{k,i}(\alpha_k, \alpha_i) }
  .
  $$
}
%
\begin{equation}
  \delta W_{i,j}(\alpha_i, \alpha_j)
  =
  W_{i,j}(\alpha_i, \alpha_j)
  - W_{i}(\alpha_i) - W_{j}(\alpha_j)
  ,
  \notag
  %\label{eq:dWij}
\end{equation}
%
or, for a triplet, $\{i, j, k\}$,
%
\begin{align*}
  &\delta W_{i,j,k}(\alpha_i, \alpha_j, \alpha_k)
  =
  W_{i,j,k}(\alpha_i, \alpha_j, \alpha_k)
  \notag\\
  &\quad
  - W_{i,j}(\alpha_i, \alpha_j)
  - W_{j,k}(\alpha_j, \alpha_k)
  - W_{k,i}(\alpha_k, \alpha_i)
  \notag\\
  &\quad
  + W_i(\alpha_i)
  + W_j(\alpha_j)
  + W_k(\alpha_k)
  .
  %\label{eq:dWijk}
\end{align*}




Without a priori assumption on the magnitudes of the corrections,
Eqs.~\eqref{eq:WI_dWJ} and \eqref{eq:dWJ_WI} are exact relations
that serve only as the mathematical definition of $\delta W_I$.
%
However, its practical value lies in that in many cases
with largely independent DOFs,
we can often assume
that the corrections of larger subsets are negligible
so that a high-order PMF of many DOFs can be approximately constructed
from a linear superposition of a few low-order ones.


If we can assume the corrections, $\delta W_J$'s,
for orders greater than $k$ are negligible,
%
\begin{equation}
  \delta W_J(\alpha_J) = 0
  \qquad
  \mbox{for $|J| \ge k$}
  ,
  \notag
  %\label{eq:mie_trunc}
\end{equation}
%
we can then approximate Eq.~\eqref{eq:WI_dWJ} as
%
\begin{equation}
  W_I(\alpha_I)
  =
  \sum_{ \substack{J \subseteq I, \; |J| \le k} }
  \!\! \delta W_J(\alpha_J)
  .
  \label{eq:WI_dWJ_MIE}
\end{equation}
%
Since Eq.~\eqref{eq:WI_dWJ_MIE}
is equivalent to Eq.~\eqref{eq:WI_dWJ}
for $|I| \le k$,
the inversion is still given by Eq.~\eqref{eq:dWJ_WI} for $|J| \le k$.
%%
%\begin{equation}
%  \delta W_J(\alpha_J)
%  =
%  \begin{dcases}
%    \sum_{ I \subseteq J }
%    (-1)^{|J| - |I|} W_I(\alpha_I)
%    &\mbox{for $|J| \le k$}
%    ,
%    \\
%    0
%    &\mbox{for $|J| > k$}
%    .
%  \end{dcases}
%  \label{eq:dWJ_WI_MIE}
%\end{equation}

In practice, we will treat the lower-order $W_I(\alpha_I)$'s
for $|I| \le k$ as the independent variables,
which are to be estimated from the data collected in simulation trajectories,
%
and use them to deduce the corrections,
$\delta W_J(\alpha_J)$'s of $|J| \le k$ from Eq.~\eqref{eq:dWJ_WI}.
%
Then, we can use these lower-order corrections
to construct higher-order PMFs, $W_I(\alpha_I)$'s,
by superposition using Eq.~\eqref{eq:WI_dWJ_MIE}.

For example,
in the second-order approximation,
we assume that
$\delta W_{i, j, k} = \delta W_{i,j,k, l} = \cdots = 0$.
%
Then,
%$\delta W_{i, j} = W_{i,j} - W_i - W_j$,
$$\delta W_{i, j}(\alpha_i, \alpha_j)
= W_{i,j}(\alpha_i, \alpha_j) - W_i(\alpha_i) - W_j(\alpha_j),$$
and the triplet PMF is expressed as a superposition
of the pair and marginal PMFs according to Eq.~\eqref{eq:W_ijk},
%$$
%W_{i, j, k} = W_{i,j} + W_{j,k} + W_{k,i} - W_i - W_j - W_k.
%$$
\begin{align*}
W_{i,j,k}(\alpha_i, \alpha_j, \alpha_k)
=
W_{i,j}(\alpha_i, \alpha_j) + W_{j,k}(\alpha_j, \alpha_k)\\
  + W_{k,i}(\alpha_k, \alpha_i)
  - W_i(\alpha_i) - W_j(\alpha_j) - W_k(\alpha_k)
.
\end{align*}
%
Written in terms of the distributions, $p_I(\alpha_I)$'s,
[cf. Eq.~\eqref{eq:WI_def}], the above relation gives
%
$$
p_{i,j,k}(\alpha_i, \alpha_j, \alpha_k)
=
\frac{ p_{i,j}(\alpha_i, \alpha_j) \, p_{j,k}(\alpha_j, \alpha_k) \, p_{k,i}(\alpha_k, \alpha_i) }{ p_i(\alpha_i) \, p_j(\alpha_j) \, p_k(\alpha_k) }
,
$$
recovers the classical version of the superposition approximation\cite{kirkwood1935, born1946}.
\note{Eq.~(4.6) in \cite{born1946}.}

\subsection{Mutual information expansion (MIE)}


Using Eq.~\eqref{eq:WI_dWJ_MIE} in Eq.~\eqref{eq:S_W},
we get an expression for the entropy for the $k$th-order MIE,
\begin{equation}
  \frac{S}{k_B}
  =
  \sum_{|J| \le k} \overline{ \delta W_J(\alpha_J) },
  \label{eq:ent_MIE1}
\end{equation}
where $J$ goes through all subsets of $\{1, \dots, n\}$ satisfying $|J| \le k$,
and $\delta W_J(\alpha_J)$ can be computed from Eq.~\eqref{eq:dWJ_WI}.

Conventionally, the sum in Eq.~\eqref{eq:ent_MIE1}
is arranged by the subset size as
\begin{equation}
  \frac{S}{k_B}
  =
  \sum_{j = 1}^k (-1)^{j-1} \MI_j
  ,
  \label{eq:S_MIE}
\end{equation}
%
where the $j$th-order mutual information, $\MI_j$, is defined as
\begin{align}
  \MI_j
  &\equiv (-1)^{j-1} \sum_{|J| = j} \overline{ \delta W_J(\alpha_J) }
  \notag \\
  &=
  \sum_{ |I| \le j }
  (-1)^{|I| - 1}
  {n - |I| \choose j - |I|}
  \, \frac{S_I}{k_B}
  ,
  \label{eq:MI_comb}
\end{align}
where we have used Eq.~\eqref{eq:dWJ_WI}
in the second step,
and we have defined, in analogous to Eq.~\eqref{eq:S_W},
the entropy of the subset $I$ as
$S_I / k_B \equiv \overline{ W_I(\alpha_I) }$.
%$$
%\frac{ S_I } { k_B } \equiv \overline{ W_I(\alpha_I) }.
%$$
%
Thus, Eq.~\eqref{eq:MI_comb} shows that
the mutual information is a linear combination of
the entropies of the subsets.

%Using Eq.~\eqref{eq:dWJ_WI}, we have for $j \le k$,
%\begin{align}
%  \MI_j
%  &=
%  \sum_{ |J| = j }
%  \sum_{ I \subseteq J }
%  (-1)^{|I| - 1}
%  \, \overline{ W_I(\alpha_I) }
%  \notag \\
%  &=
%  \sum_{ |I| \le j }
%  (-1)^{|I| - 1}
%  {n - |I| \choose j - |I|}
%  \, \overline{ W_I(\alpha_I) }
%  ,
%  \label{eq:MIj_expansion}
%\end{align}
%%
%where the appearance of the binomial coefficient
%is due to the number of ways of forming
%a size-$j$ superset of $I$,
%which is given by the number of ways of choosing
%$j - |I|$ DOFs out of the $n - |I|$ remaining DOFs.
%
%Using Eq.~\eqref{eq:MIj_expansion} in Eq.~\eqref{eq:S_MIE} yields
%%
%\begin{align}
%  \frac{S}{k_B}
%  &=
%  \sum_{j=1}^k
%  \sum_{|I| \le j}
%  (-1)^{j - |I|}
%  {n - |I| \choose j - |I|}
%  \, \overline{ W_I(\alpha_I) }
%  \notag\\
%  &=
%  \sum_{|I| \le k}
%  (-1)^{ k - |I| }
%  {n - |I| - 1 \choose k - |I|}
%  \, \overline{ W_I(\alpha_I) }
%  ,
%  \notag
%  %\label{eq:S_MIE_expansion}
%\end{align}
%%
%where we have used the combinatorial identity
%$$
%\sum_{r = 0}^R (-1)^r \, {m \choose r}
%=
%(-1)^R \, {m - 1 \choose R},
%$$
%where $0 \le R \le m$, and we have defined ${-1 \choose 0} \equiv 1$.
%\note{ See, for example, \url{https://en.wikipedia.org/wiki/Binomial\_coefficient\#Partial\_sums} }


If we estimate $S_I$ using the direct counting method,
by counting the occurrences in the subspace of $\alpha_I$ as
$$
\hat S_I = - k_B \sum_{\alpha_I} \hat p(\alpha_I) \, \ln \hat p(\alpha_I),
$$
the result, $\hat S_I$, would also suffer from the bias discussed
in Sec.~\ref{sec:fsbias}, and thus should be corrected accordingly.
%
As the subspace of $\alpha_I$ is the direct product of
the individual spaces of all elements in $I$,
its size, $M_I = \prod_{i \in I} M_i$
(with $M_i$ being the number of possibilities for the $i$th DOF),
grows exponentially with the size of $I$.
%
Thus, according to Eq.~\eqref{eq:Shat_ave},
the bias in uncorrected result would be much more noticeable
in a higher-order version of MIE.


\appendix

\section{\label{sec:inclexcl}
Proof of the inversion formula }

We can verify Eq.~\eqref{eq:dWJ_WI}
by a direct expansion using Eq.~\eqref{eq:WI_dWJ}.
%
Consider a particular subset $K \subseteq J$.
Let us compute the contribution of $\delta W_K$
to the right-hand side of Eq.~\eqref{eq:dWJ_WI}.
%
Note that $\delta W_K$ contributes only to those terms
whose $I$ is a superset of $K$,
and the contribution coefficient is $(-1)^{|J|-|I|}$.
%
If $K$ is strict subset of $J$,
then there is at least one component $j^*$ that belongs to $J$
but not to $K$,
and all supersets, $I$'s,
satisfying the above condition, and that given by Eq.~\eqref{eq:dWJ_WI},
$K \subseteq I \subseteq J$,
can be partitioned to two equally populated groups,
according to whether the $I$ has $j^*$ or not.
%
However, because the corresponding members of the two groups
contribute the same value with opposite signs to the sum,
the total contribution is zero.
%
This means $\delta W_K$ can contribute to the right-hand side
if and only if $K = J$, and the contribution is $1 \cdot W_J$,
as intended.

%If we limit ourselves to supersets, $I$'s, of size $i$,
%then there are
%${|J| - |K| \choose i - |K|}$ ways
%of including components other than those in $K$ into $I$,
%and the total contribution is
%$(-1)^{|J|-i} {|J| - |K| \choose i - |K|}$.
%%
%Summing over the possible sizes yields
%$$
%\sum_{j=|K|}^{|J|} (-1)^{|J|-i} {|J| - |K| \choose i - |K|}
%= (1 - 1)^{|J| - |K|} = \delta_{|J|, |K|}.
%$$
%This shows that the total contribution of $\delta W_K$
%to the right-hand side of Eq.~\eqref{eq:dWJ_WI}
%vanishes unless $K = J$,
%which is the intended result.

\bibliography{simul}
\end{document}

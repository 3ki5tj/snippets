\documentclass[reprint, superscriptaddress]{revtex4-1}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{upgreek}
\usepackage[table,usenames,dvipsnames]{xcolor}
%\usepackage{tikz}
%\usetikzlibrary{calc}
\usepackage{hyperref}
\usepackage{setspace}

\setcitestyle{super}

\hypersetup{
  colorlinks,
  linkcolor={blue!80!black},
  citecolor={green!20!black},
  urlcolor={blue!80!black}
}


\definecolor{DarkBlue}{RGB}{0,0,64}
\definecolor{DarkBrown}{RGB}{64,20,10}
\definecolor{DarkGreen}{RGB}{0,64,0}
\definecolor{DarkPurple}{RGB}{128,0,84}
\definecolor{LightGray}{gray}{0.85}
% annotation macros
\newcommand{\repl}[2]{{\color{gray} [#1] }{\color{blue} #2}}
\newcommand{\add}[1]{{\color{blue} #1}}
\newcommand{\del}[1]{{\color{gray} [#1]}}
\newcommand{\note}[1]{{\color{DarkGreen}\footnotesize \textsc{Note.} #1}}

\newcommand{\MI}{\mathcal I} % mutual information

\begin{document}
\title{Bias of the configuration entropy estimated from counting-based methods}

%\author{Justin A. Drake}
%\affiliation{
%Sealy Center for Structural Biology and Molecular Biophysics,
%The University of Texas Medical Branch,
%Galveston, Texas 77555-0304, USA}
%\author{Danielle D. Seckfort}
%\affiliation{
%Quantitative and Computational Biosciences,
%Baylor College of Medicine, Houston, Texas 77030, USA}
%\author{Omneya Nassar}
%\affiliation{
%Sealy Center for Structural Biology and Molecular Biophysics,
%The University of Texas Medical Branch,
%Galveston, Texas 77555-0304, USA}
%\author{B. Montgomery Pettitt}
%\email{mpettitt@utmb.edu}
%\affiliation{
%Sealy Center for Structural Biology and Molecular Biophysics,
%The University of Texas Medical Branch,
%Galveston, Texas 77555-0304, USA}
%\affiliation{
%Quantitative and Computational Biosciences,
%Baylor College of Medicine, Houston, Texas 77030, USA}

\begin{abstract}
We show that the configuration entropy estimated from methods based on counting occurrences
carries a negative bias
that is roughly inversely proportional to the simulation length.
%
Here we propose a simple correction to the bias
and discuss its application to the mutual information expansion method.
\end{abstract}

\maketitle


\section{Introduction}

Entropy is a key concept in statistical mechanics.
%
It provides a quantitative measure of
the number of possible states a system can go through during its time evolution.
%
In particular, we can define the configuration entropy,
for $M$ coarse-grained states in the configuration space, as
%
\begin{equation}
  S
  \equiv
  -k_B \sum_{\alpha = 1}^M p(\alpha) \, \ln p(\alpha)
  ,
  \label{eq:entropy_def}
\end{equation}
%
where $p(\alpha)$ is the probability of discrete state $\alpha = 1, \dots, M$,
and $k_B$ is the Boltzmann constant.
%
For the even distribution, $p(\alpha) \equiv 1/M$,
we can readily verify that $S \equiv k_B \, \ln M$
is proportional to the logarithm (hence a measure) of the number of states,
whereas Eq.~\eqref{eq:entropy_def}
provides a natural extension of the measure to nonuniform distributions.

We can readily apply Eq.~\eqref{eq:entropy_def} to a trajectory
obtained from a molecular dynamics (MD) or Monte Carlo (MC) simulation,
with the probabilities, $p(\alpha)$'s,
estimated from the observed occurrences of the states.
%
This straightforward application also furnishes the basis
of many advanced counting-based methods for estimating the configuration entropy\cite{hnizdo2007, killian2007}.
%
Among them, the method of mutual information expansion (MIE)\cite{killian2007} is notable
for its potential to handle a vast but decomposable state space common in complex systems.
%
The MIE method is suitable for a many-body system
whose degrees of freedom (DOFs) are only loosely coupled.
%
Starting from the sum of marginal entropies of individual DOFs,
the method offers a cascade of levels of corrections
to systematically absorb the contributions of higher-order correlations
of increasingly larger groups, such as pairs, triplets, quartets, etc.

However, the above counting-based methods
tend to underestimate the configuration entropy
because a realistic simulation of finite length
often cannot fully sample the state space.
%
Thus, it is useful to understand the nature of this bias
and its dependence on the sample size.
%
With this understanding,
we hope to derive a suitable finite-size correction.

Here we show that the configuration entropy derived
from the counting-based methods (including the MIE method)
generally carries a negative bias
that roughly scales inversely with the trajectory length.
%
We also propose a blocking procedure
that can readily remove the bias
to the leading order.
%
%In addition, the bias of the MIE method, in particular,
%can grow dramatically with the order of tuples.
%
%In some cases, we can reduce the magnitude of the bias
%by imposing some restrictions on the higher-order tuples included.



\section{Theory and methods}

\subsection{\label{sec:fsbias}
Finite-size bias in the direct counting method}

We will first discuss the finite-size bias associated with
the direct application of Eq.~\eqref{eq:entropy_def},
which will be referred to as the direct counting method below.
%
Here, we estimate the probability, $p(\alpha)$,
by the frequency, $\hat{p}(\alpha) = \hat{n}(\alpha) / t$,
where $\hat{n}(\alpha)$ is the number of times that state $\alpha$ occurs in the trajectory of $t$ steps.
%
Then, the entropy estimate is
%
\begin{equation}
  \hat S
  =
  -k_B \sum_{\alpha = 1}^M \hat{p}(\alpha) \, \ln \hat{p}(\alpha)
  .
  \label{eq:entropy_est}
\end{equation}
%
To investigate the error of Eq.~\eqref{eq:entropy_est},
we will introduce an ensemble of replica systems,
each of which undergoes an independent simulation.
%
We can then define the error of the entropy estimate
from the difference
between the ensemble average,
$\bigl\langle \hat S \bigr\rangle$,
and the true value, $S$.
%
By the series expansion of $\hat{p}(\alpha)$ around $p(\alpha)$, we have
%
\begin{align}
  \hat S
  =
  S
  &- k_B \sum_{\alpha = 1}^M
    \bigl[\ln p(\alpha) + 1 \bigr]
    \bigl[ \hat{p}(\alpha) - p(\alpha) \bigr]
  \notag\\
  &- k_B \sum_{\alpha = 1}^M
    \frac{ \bigl[ \hat{p}(\alpha) - p(\alpha) \bigr]^2 } { 2 \, p(\alpha) }
  .
  \notag
  %\label{eq:Shat_series}
\end{align}
%
\note{The series expansion of the function, $h(x) = -x \, \ln x$,
around $x_0$,
%
\begin{align*}
  h(x)
  &\approx h(x_0) + h'(x_0) (x - x_0) + \frac{h''(x_0)}{2} (x - x_0)^2 \\
  &\approx h(x_0) - (\ln x_0 + 1) ( x - x_0) - \frac{1}{2 \, x_0} (x - x_0)^2
  .
\end{align*}
}
%
Assuming the sampling is unbiased,
we have $\langle \hat{p}(\alpha) \rangle = p(\alpha)$,
and the linear term vanishes
after ensemble averaging, and
%
\begin{align}
  \bigl\langle \hat S \bigr\rangle
  \approx
  S - k_B \sum_{\alpha = 1}^M
    \frac{ \operatorname{var} \hat{p}(\alpha) } { 2 \, p(\alpha) }
  =
  S - k_B \sum_{\alpha = 1}^M
    \frac{ \operatorname{var} \hat{n}(\alpha) } { 2 \, p(\alpha) \, t^2 }
  ,
  \label{eq:entest_2nd}
\end{align}
%
which shows that the quadratic term in the expansion
represents a nonpositive bias to the leading order.

Next we will show that the bias is inversely proportional
to the number of steps.
%
By definition, we have
%
\begin{equation*}
  \hat{n}(\alpha) = \sum_{t' = 1}^t \delta_{\alpha, \alpha(t')},
\end{equation*}
%
where $\alpha(t')$ denotes the state at step $t'$,
and $\delta_{\alpha, \gamma}$ is the Kronecker delta,
which takes the value of $1$ if $\alpha = \gamma$, or $0$ otherwise.
%
We can readily show that
\begin{align*}
  \bigl\langle \hat{n}(\alpha) \bigr\rangle
  &=
  t \, p(\alpha), \\
  %
  \operatorname{var}{\hat{n}(\alpha)}
  &=
  %\left\langle \hat{n}(\alpha)^2 \right\rangle - \langle \hat{n}(\alpha) \rangle^2
  %\\
  %&=
  t \, p(\alpha) \, \bigl( 1 - p(\alpha) \bigr) \, \bigl(2 \, \tau_\alpha + 1\bigr)
  ,
\end{align*}
%
where $\tau_\alpha$ is the integral autocorrelation time of $\delta_{\alpha, \alpha(t)}$.\footnote{
The autocorrelation function is defined as
$$
\kappa_\alpha(t) = \bigl(\delta_{\alpha, \alpha(t)} - p(\alpha)\bigr)
\bigl(\delta_{\alpha, \alpha(0)} - p(\alpha)\bigr),
$$
and the autocorrelation time is given by
$\tau_\alpha = \sum_{t = 1}^\infty \kappa_\alpha(t)/\kappa_\alpha(0)$.
}
%
Thus, we can rewrite Eq.~\eqref{eq:entest_2nd} as
%
\begin{align}
  \bigl\langle \hat S \bigr\rangle
  &\approx
  S - \frac{ C } { t }
  %\notag\\
  %&\approx
  %S - k_B
  %  \frac{ (M - 1) ( 2 \, \tau + 1) } { 2 \, t }
  ,
  \label{eq:Shat_ave}
\end{align}
%
where $C = k_B \sum_{\alpha = 1}^M
    \bigl(1 - p(\alpha)\bigr) ( 2 \, \tau_\alpha + 1)/2$.
%
For perfect sampling, $\tau_\alpha \equiv 0$, so we have $C = (M-1)/2$.
%where in the second step,
%we have assumed that all integral autocorrelation times are the same.

In summary, we have shown that the entropy estimated
from direct counting
is expected to carry a negative bias,
and, to the leading order, the bias is
inversely proportional to the trajectory length,
or sample size.



\subsection{Corrections to the linear bias}


We now propose a blocking procedure to correct the bias to the leading order.
%
We will consider the estimate from Eq.~\eqref{eq:entropy_est}
as a function of the trajectory length, $t$, and denote it as $\hat S(t)$.
%
Then, we can define an estimate, $\hat S(t_b)$,
from a trajectory segment of $t_b$ steps.
%
The ensemble average of $\hat S(t_b)$ also follows
Eq.~\eqref{eq:Shat_ave}, but with $t$ replaced by $t_b$.
%
This allows us to deduce the constants, $S$ and $C$, in Eq.~\eqref{eq:Shat_ave}
by solving the two equations for $\hat S(t)$ and $\hat S(t_b)$.
%
This process is equivalent to a two-point linear regression,
and it results in an extrapolated estimate
%
\begin{align}
    \hat S(t, t_b)
    =
    \frac{ t \, \hat S(t) - t_b \, \hat S(t_b) }
         { t - t_b }
    .
    \label{eq:S_2pt}
\end{align}

We can implement Eq.~\eqref{eq:S_2pt} in a blocking procedure.
%
We can divide the trajectory into $b$ blocks of equal length,
each of $t_b = t/b$ steps.
%
Then, we can form an estimate $S_{b, (l)}$ for each block $l = 1, \dots, b$ from Eq.~\eqref{eq:entropy_est}.
%
We then use the average of the $b$ block estimates
as the $\hat S(t_b)$ in Eq.~\eqref{eq:S_2pt}.
%
The advantage of using the block average instead of
the value from the first block is that
the former make the variance of $\hat S(t, t_b)$
roughly as low as that of $\hat S(t)$,
and thus minimizes the artifact of the extrapolation.


%\subsubsection{Higher-order improvement on the correction}
%
%Note that Eq.~\eqref{eq:Shat_ave} is only the first-order expansion
%to an infinite series,
%%
%\begin{equation}
%  \hat S = S - \frac{a_1}{t} - \frac{a_2}{t^2} - \cdots
%  .
%  \label{eq:S_series_invt}
%\end{equation}
%%
%Thus, it is possible, at least in theory,
%to further improve the estimate by a higher-order extrapolation
%against $1/t$ with more data points.
%%
%For example, if we can divide the entire trajectory to $c$ blocks
%with $c > b$ and obtain the block average of $\hat S(t_c)$
%as in the above procedure,
%%
%The three-point extrapolation estimate reads
%$$
%\hat S(t, t_b, t_c) =
%\frac{ t_b \, \hat S(t, t_b) - t_c \, \hat S(t, t_c) }
%     { t_b - t_c },
%$$
%which can eliminate not only the $1/t$ tail
%but also the higher-order $1/t^2$ tail.


%\subsubsection{Alternative extrapolation}

We can improve Eq.~\eqref{eq:S_2pt} by
fitting the function $\hat S(t)$ against a ``better'' function.
%
We note that Eq.~\eqref{eq:Shat_ave} is an asymptotic expansion
that only holds when the frequencies are close to the true probabilities,
which usually means when the effective number of samples,
$t^* = t/(\overline{2 \, \tau_\alpha + 1})$, is much greater than
the number of states $M$.
%
In initial stages, when $t^* \ll M$,
we would expect the number of states visited to be proportional to $t^*$,
and the entropy estimate, $\hat S(t)$,
to be $\ln t^* = \ln t + \mathrm{const.}$
%
A phenomenological form to accommodate both the initial and asymptotic behavior
would be
%
\begin{equation}
  \bigl\langle \hat S \bigr\rangle
  = S - \ln\left(1 + \frac{C}{t}\right)
  .
  \label{eq:expform}
\end{equation}
%
To derive a correction based on the above functional form,
we note that Eq.~\eqref{eq:expform} represents a linear relation between
$e^{-\bigl\langle \hat S \bigr\rangle}$
(instead of $\bigl\langle \hat S \bigr\rangle$ itself)
and $1/t$.
%
Thus, we can readily adapt the correction, Eq.~\eqref{eq:S_2pt},
to the exponential form as
%
\begin{equation}
  \hat S(t, t_b) = -\ln\left(
    \frac{ t \, e^{-\hat S(t)} - t_b \, e^{-\hat S(t_b)} }
         { t - t_b }
  \right)
  .
  \label{eq:S_2pt_exp}
\end{equation}
%
The linear, Eq.~\eqref{eq:S_2pt}, and exponential, Eq.~\eqref{eq:S_2pt_exp}, corrections
have a similar asymptotic behavior,
but in initial stages, the latter can sometimes
improve the accuracy of the estimate.

%As we will see, the same type of bias also exists
%in the more elaborate the MIE method.


\subsection{Mutual information expansion}


The direct counting method breaks down
for a complex system of many degrees of freedom (DOFs).
%
As the number of states increases exponentially with the number of DOFs,
the number of samples collected in each state rapidly diminishes,
which in turn deteriorates the precision of the entropy estimate.
%
For these many-body systems,
we may alternatively adopt
the mutual information expansion (MIE) method,
which is a more advanced, albeit approximate, method designed for a system
with many largely-independent DOFs.

The basic idea is that the full distribution of a multi-body system
can often be approximately derived
from the marginal and joint distributions of small groups of DOFs,
such as single DOFs, pairs, triplets, etc.
%
To the first order,
we may simply approximate the full distribution as a product
of the marginal distributions of all DOFs,
as if the DOFs were independent.
%
The correlations among pairs and triplets of DOFs
are incrementally taken into account in the second- and third-order MIE,
respectively.
%
Accordingly, the entropy of the entire system,
can be obtained from a superposition of the entropies
of the marginal and joint distributions
of small DOF groups.
%
A more detailed discussion is presented in Appendix~\ref{sec:MIE_review}.

If the entropies of the small DOF groups are estimated
using the direct counting method,
the estimate of the total entropy from MIE
naturally suffers from the bias discussed below.
%
Such a bias can be readily removed
if we apply the above corrections
to each of groups.
%
This procedure is particularly simple for the linear correction.
%
Because of the linear structure of MIE,
we can apply Eq.~\eqref{eq:S_2pt}
to the final output of MIE
instead of individually to the entropy of every DOF group.



\section{Results}

\subsection{Direct-counting method}

To test the corrections we proposed for the direct-counting method,
we performed simulations of a simple random walker.
%
In each step, the random walker can assume any of the $M$ states
with equal probability without any memory.
%
The entropy of the system is simply $k_B \, \ln M$,
and we wish to see how the entropy estimated
from Eq.~\eqref{eq:entropy_est} converges to the true value,
and how the corrections accelerate the convergence.
%
Despite its simplicity, the model may bear relevance
to the sampling of metastable states of a glassy biomolecule,
which is frustrated by many moderate energetic barriers
separating metastable states.

\begin{figure}[h]\centering
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/walk_q1e3.pdf}
  }
  \caption{
    \label{fig:walk_q1e3}
    Estimated entropy, $\hat S$,
    versus simulation time, $t$, for a random walk over $M = 10^3$ states.
    %
    The results have been averaged over $10^5$ independent trajectories.
    The lines are a guide for the eye.
  }
\end{figure}

As shown in Fig.~\ref{fig:walk_q1e3}, for a walker of $M = 10^3$ states,
both the linear and exponential corrections,
Eqs.~\eqref{eq:S_2pt} and \eqref{eq:S_2pt_exp},
improved the convergence.
%
The errors of the entropy estimates are shown in the inset.
%
The uncorrected result
indeed carried an error roughly proportional to the inverse simulation time,
and both corrections removed the asymptotic error.
%
The exponential correction was more advantageous
at the beginning of the simulation,
whereas the linear one performed better at long times.
%
This suggests that the exponential correction
can offer greater accuracy than the linear one,
especially in initial stages.


\begin{figure}[h]\centering
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/walk_Svsq.pdf}
  }
  \caption{
    \label{fig:walk_Svsq}
    Estimated entropy, $\hat S$,
    against the number of states
    after $t = 10^3$ steps.
    %
    The results have been averaged over independent trajectories.
    The lines are a guide for the eye.
  }
\end{figure}

To further investigate the difference between the two corrections,
we performed simulations with different number states, $M$,
and plotted the estimated entropy against $M$
after $t = 10^3$ steps in Fig.~\ref{fig:walk_Svsq}.
%
Roughly speaking, the exponential correction performed better
when the number of states is greater than the number of steps
(i.e. in initial stages of a simulation),
and it allows a reasonable extension to
a number of states roughly $10$-times as great as the number of steps.
%
But for larger numbers of states, all methods would fail.
%
This comparison also suggests that there can be
better functional form than the linear or exponential ones used here
for correcting the entropy estimate in initial stages.



\subsection{MIE method on a linear Potts model}

We further tested the corrections for the MIE method
on a linear Potts model,
which is a linear chain of $n$ identical spins
that interact with only their nearest neighbors.
%
We wish to use the simplistic model
to capture the dihedral conformation variety
observed in disordered or flexible peptides.\note{cite Drake,Pettitt 2018}
%
Each spin, representing the dihedral conformation of an amino-acid residue,
can take one of the $q = 6$ different states with equal probability.
%
The interaction between two neighboring spins
is such that there is a favorable energetic contribution of $-J$
when the two spins assume the same state, or none otherwise.
%
A more detailed description of the model is given in Appendix~\ref{sec:Potts}.
%
Note that although non-neighboring spins have no direct coupling,
they are still correlated though successive or higher-order coupling,
especially at a low temperature.
%
In our test, the number of spins is $n=10$,
and the temperature is $T = 0.5$.
%
We will report our results in reduced units (with $k_B = J = 1$).


The results from the second-order MIE
(which takes into account the correlations of pairs,
but not those of triplets or larger groups)
are shown in Fig.~\ref{fig:potts_mie2nd}.
%
The error of the uncorrected estimate
scaled roughly inversely with the simulation time, $t$,
as can be seen from the inset.
%
The linear correction largely removed the inverse-time bias
and improved the entropy estimate significantly.
%
However, the remaining error dropped more slowly than $t^{-2}$.
%
This might be due to an implementation detail:
we imposed that for any pair of spins,
the entropic correction,
$\delta S_{i, j}$ (cf. Appendix~\ref{sec:MIE_review}),
has to be nonpositive,
otherwise, zero will be used.
%
The exponential correction further improved over
the linear one in the initial stages,
while the improvement in the asymptotic regime is negligible.
%

\begin{figure}[h]\centering
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/potts_mie2nd.pdf}
  }
  \caption{
    \label{fig:potts_mie2nd}
    Estimated entropy from the second-order MIE method
    versus simulation time on the linear Potts model.
    %
    The reference value is the theoretical expectation
    of the second-order MIE method.
    %
    Inset: error of the estimated entropy
    versus simulation time.
    %
    The results have been averaged over $1000$ independent trajectories.
    The lines are a guide for the eye.
  }
\end{figure}


For the third-order MIE,
the linear correction again improved the entropy estimate
significantly.
However, the exponential correction was less satisfactory.
%
Unlike in the second-order case,
we can no longer ascertain the sign of
the third-order correction for each triplet,
$\delta S_{i, j, k}$ (see Appendix~\ref{sec:MIE_review});
and thus the estimates on triplet entropies
were more susceptible to random errors,
especially for the exponential correction.
%
On balance, it appeared to be safer
to use the linear extrapolation for the MIE method,
at least for the third or higher order.

\begin{figure}[h]\centering
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/potts_mie3rd.pdf}
  }
  \caption{
    \label{fig:potts_mie3rd}
    Estimated entropy from the third-order MIE method
    versus simulation time on the linear Potts model.
    %
    The reference value is the theoretical expectation
    of the third-order MIE method.
    %
    Inset: error of the estimated entropy
    versus simulation time.
    %
    The results have been averaged over $1000$ independent trajectories.
    The lines are a guide for the eye.
  }
\end{figure}

\section{Conclusions}

We have shown that the configuration entropy estimated from counting-based methods
generally carries a negative bias whose magnitude
is roughly inversely proportional to the simulation length.
%
Based on this observation, we have proposed a simple blocking procedure
to remove the bias to the leading order.
%
We hope the findings presented here to be useful
for the biophysical community
in investigating the thermodynamic properties of
biologically important systems.


\appendix

\section{\label{sec:MIE_review}
Mutual information expansion}

The MIE method is based on the idea of
the Kirkwood superposition approximation (KAS)\cite{kirkwood1935, born1946},
or that of interaction information\cite{mcgill1954}.
%
Below, we will discuss the idea and its application to MIE.

\subsection{\label{sec:Kirkwood}
Kirkwood superposition approximation}

We first recall that the (dimensionless) potential of mean force (PMF)
is defined for a distribution, $p(\alpha)$,
as
$$
W(\alpha) = -\ln p(\alpha) = \ln\left[ \frac{1}{p(\alpha)} \right].
$$
The PMF can be interpreted as
the free energy of confining the ensemble to a single state,
or the amount of work
(in the unit of $k_B \, T$'s, with $T$ being the temperature)
required to transfer an average system randomly drawn from the ensemble (of probability $1$)
to state $\alpha$ [of probability $p(\alpha)$].
%
We can now rewrite Eq.~\eqref{eq:entropy_def} as
%
\begin{equation}
  \frac{S}{k_B}
  =
  \sum_{\alpha = 1}^M p(\alpha) \, W(\alpha)
  =
  \overline{ W(\alpha) }
  ,
  \label{eq:S_W}
\end{equation}
%
which shows that the configuration entropy
is the mean PMF.
%
The superposition approximation is based on the intuition
that for a physical or extensive system composed of
multiple loosely-coupled DOFs,
the PMF of the entire system is largely
the sum of the marginal PMFs of individual DOFs.

Consider a many-body system of $n$ DOFs.
%
We label the DOFs numerically, $1, \dots, n$,
and specify the state as $\alpha = (\alpha_1, \dots, \alpha_n)$.
%
%For simplicity, we will assume that the degrees of freedom are identical.
%
For a subset of DOFs, $I = \{i, j, \dots\}$,
we denote the joint distribution by $p_I(\alpha_I)$,
and define the joint PMF as
%
\begin{equation}
  W_I(\alpha_I) = -\ln p_I(\alpha_I)
  .
  \label{eq:WI_def}
\end{equation}
%
For example,
the marginal PMF of a single DOF $i$ is given by $W_i$ (omitting the argument),
the joint PMF of a pair $\{i, j\}$ by $W_{i, j}$
(dropping the braces ``$\{$'' and ``$\}$'' in the subscript, same below),
that of a triplet $\{i, j, k\}$ by $W_{i, j, k}$, etc.

Now by assuming that the DOFs are largely independent,
we can write the PMF of a pair $\{i, j\}$
as the sum of the marginal PMFs of the two individual DOFs $i$ and $j$
and a hopefully small two-body correction, $\delta W_{i,j}$,
%
\begin{equation}
  W_{i,j}(\alpha_i, \alpha_j)
  =
  W_i(\alpha_i) + W_j(\alpha_j)
  + \delta W_{i,j}(\alpha_i, \alpha_j)
  .
  \label{eq:W_ij}
\end{equation}
%
Similarly, for a triplet, $\{i, j, k\}$,
we can define the three-body correction, $\delta W_{i,j,k}$, via
%
\begin{align}
  &W_{i,j,k}(\alpha_i, \alpha_j, \alpha_k)
  =
  W_i(\alpha_i) + W_j(\alpha_j) + W_k(\alpha_k)
  \notag \\
  &\quad
  + \delta W_{i,j}(\alpha_i, \alpha_j)
  + \delta W_{j,k}(\alpha_j, \alpha_k)
  + \delta W_{k,i}(\alpha_k, \alpha_i)
  \notag \\
  &\quad
  + \delta W_{i,j,k}(\alpha_i, \alpha_j, \alpha_k)
  .
  \label{eq:W_ijk}
\end{align}
%


Generally, we can define the PMF correction, $\delta W_I$,
for an arbitrary set of DOFs, $I$,
implicitly through the recurrence relation,
%
\begin{equation}
  W_I(\alpha_I)
  =
  \sum_{J \subseteq I}
  \delta W_J(\alpha_J)
  ,
  \label{eq:WI_dWJ}
\end{equation}
%
where the sum goes through all subsets $J$'s of $I$,
and we have defined
$\delta W_\emptyset \equiv W_\emptyset \equiv 0$
for the empty set $\emptyset$.
%
Note that in this definition,
$\delta W_i \equiv W_i$
for a single DOF $i$.
%
We can readily verify that
Eqs.~\eqref{eq:W_ij} and \eqref{eq:W_ijk}
are special cases of Eq.~\eqref{eq:WI_dWJ}

As Eq.~\eqref{eq:WI_dWJ} is a linear relation,
by using the inclusion-exclusion principle\cite{bjorklund2007},
we can invert it to get an explicit definition of $\delta W_J$\footnote{
  We can verify Eq.~\eqref{eq:dWJ_WI}
  by a direct expansion using Eq.~\eqref{eq:WI_dWJ}.
  %
  Consider a particular subset $K \subseteq J$.
  Let us compute the contribution of $\delta W_K$
  to the right-hand side of Eq.~\eqref{eq:dWJ_WI}.
  %
  Note that $\delta W_K$ contributes only to those terms
  whose $I$ is a superset of $K$,
  and the contribution coefficient is $(-1)^{|J|-|I|}$.
  %
  If $K$ is strict subset of $J$,
  then there is at least one component $j^*$ that belongs to $J$
  but not to $K$,
  and all supersets, $I$'s,
  satisfying the above condition, and that given by Eq.~\eqref{eq:dWJ_WI},
  $K \subseteq I \subseteq J$,
  can be partitioned to two equally populated groups,
  according to whether the $I$ has $j^*$ or not.
  %
  However, because the corresponding members of the two groups
  contribute the same value with opposite signs to the sum,
  the total contribution is zero.
  %
  This means $\delta W_K$ can contribute to the right-hand side
  if and only if $K = J$, and the contribution is $1 \cdot W_J$,
  as intended.
}
%
\begin{equation}
  \delta W_J(\alpha_J)
  =
  \sum_{I \subseteq J}
  (-1)^{|J| - |I|}
  W_I(\alpha_I)
  ,
  \label{eq:dWJ_WI}
\end{equation}
%
where $|I|$ denotes the size of subset $I$.
%
Below we will refer to the subset sizes, $|I|$ and $|J|$,
as the orders of $W_I(\alpha_I)$ and $\delta W_J(\alpha_J)$.
%
For example, we have, for a pair, $\{i, j\}$,\footnote{
  Just as we define the PMFs, $W_I$'s, from the joint distributions
  through Eq.~\eqref{eq:WI_def},
  we can associate the corrections, $\delta W_I$'s,
  with the (excess) correlation functions, $g_I$'s, as
  %
  $$
  g_I(\alpha_I) \equiv e^{-\delta W_I(\alpha_I)}
  .
  $$
  %
  This definition is consistent with the usual definition
  of the pair correlation function (as in liquid state theory\cite{hansen}),
  $g_{i,j}(\alpha_i, \alpha_j)$ for a pair, $\{i, j\}$,
  $$
    g_{i,j}(\alpha_i, \alpha_j)
    =
    \frac{ p_{i,j}(\alpha_i, \alpha_j) } { p_i(\alpha_i) \, p_j(\alpha_j) }
    .
  $$
  Similarly, for a triplet, $\{i, j, k\}$, we have
  $$
  g_{i,j,k}(\alpha_i, \alpha_j, \alpha_k)
  =
  \frac{ p_{i,j,k}(\alpha_i, \alpha_j \, \alpha_k) \, p_i(\alpha_i) \, p_j(\alpha_j) \, p_k(\alpha_k) }{ p_{i,j}(\alpha_i, \alpha_j) \, p_{j,k}(\alpha_j, \alpha_k) \, p_{k,i}(\alpha_k, \alpha_i) }
  .
  $$
}
%
\begin{equation}
  \delta W_{i,j}(\alpha_i, \alpha_j)
  =
  W_{i,j}(\alpha_i, \alpha_j)
  - W_{i}(\alpha_i) - W_{j}(\alpha_j)
  ,
  \label{eq:dWij}
\end{equation}
%
or, for a triplet, $\{i, j, k\}$,
%
\begin{align}
  &\delta W_{i,j,k}(\alpha_i, \alpha_j, \alpha_k)
  =
  W_{i,j,k}(\alpha_i, \alpha_j, \alpha_k)
  \notag\\
  &\quad
  - W_{i,j}(\alpha_i, \alpha_j)
  - W_{j,k}(\alpha_j, \alpha_k)
  - W_{k,i}(\alpha_k, \alpha_i)
  \notag\\
  &\quad
  + W_i(\alpha_i)
  + W_j(\alpha_j)
  + W_k(\alpha_k)
  .
  \label{eq:dWijk}
\end{align}




Without a priori assumption on the magnitudes of the corrections,
Eqs.~\eqref{eq:WI_dWJ} and \eqref{eq:dWJ_WI} are exact relations
that serve only as the mathematical definition of $\delta W_I$.
%
However, its practical value lies in that in many cases
with largely independent DOFs,
we can often assume
that the corrections of larger subsets are negligible
so that a high-order PMF of many DOFs can be approximately constructed
from a linear superposition of a few low-order ones.


If we can assume the corrections, $\delta W_J$'s,
for orders greater than $k$ are negligible,
%
\begin{equation}
  \delta W_J(\alpha_J) = 0
  \qquad
  \mbox{for $|J| \ge k$}
  ,
  \notag
  %\label{eq:mie_trunc}
\end{equation}
%
we can then approximate Eq.~\eqref{eq:WI_dWJ} as
%
\begin{equation}
  W_I(\alpha_I)
  =
  \sum_{ \substack{J \subseteq I, \; |J| \le k} }
  \!\! \delta W_J(\alpha_J)
  .
  \label{eq:WI_dWJ_MIE}
\end{equation}
%
Since Eq.~\eqref{eq:WI_dWJ_MIE}
is equivalent to Eq.~\eqref{eq:WI_dWJ}
for $|I| \le k$,
the inversion is still given by Eq.~\eqref{eq:dWJ_WI} for $|J| \le k$.
%%
%\begin{equation}
%  \delta W_J(\alpha_J)
%  =
%  \begin{dcases}
%    \sum_{ I \subseteq J }
%    (-1)^{|J| - |I|} W_I(\alpha_I)
%    &\mbox{for $|J| \le k$}
%    ,
%    \\
%    0
%    &\mbox{for $|J| > k$}
%    .
%  \end{dcases}
%  \label{eq:dWJ_WI_MIE}
%\end{equation}

In practice, we will treat the lower-order $W_I(\alpha_I)$'s
for $|I| \le k$ as the independent variables,
which are to be estimated from the data collected in simulation trajectories,
%
and use them to deduce the corrections,
$\delta W_J(\alpha_J)$'s of $|J| \le k$ from Eq.~\eqref{eq:dWJ_WI}.
%
Then, we can use these lower-order corrections
to construct higher-order PMFs, $W_I(\alpha_I)$'s,
by superposition using Eq.~\eqref{eq:WI_dWJ_MIE}.

For example,
in the second-order approximation,
we assume that
$\delta W_{i, j, k} = \delta W_{i,j,k, l} = \cdots = 0$.
%
Then, $\delta W_{i,j}$ is given by Eq.~\eqref{eq:dWij}.
The triplet PMF is expressed as a superposition
of the pair and marginal PMFs according to Eq.~\eqref{eq:W_ijk},
%$$
%W_{i, j, k} = W_{i,j} + W_{j,k} + W_{k,i} - W_i - W_j - W_k.
%$$
\begin{align*}
W_{i,j,k}(\alpha_i, \alpha_j, \alpha_k)
=
W_{i,j}(\alpha_i, \alpha_j) + W_{j,k}(\alpha_j, \alpha_k)\\
  + W_{k,i}(\alpha_k, \alpha_i)
  - W_i(\alpha_i) - W_j(\alpha_j) - W_k(\alpha_k)
.
\end{align*}
%
This result can also be obtained from Eq.~\eqref{eq:dWijk} with $\delta W_{i,j,k}$ set to zero.
%
Written in terms of the distributions, $p_I(\alpha_I)$'s,
[cf. Eq.~\eqref{eq:WI_def}], the above relation gives
%
$$
p_{i,j,k}(\alpha_i, \alpha_j, \alpha_k)
=
\frac{ p_{i,j}(\alpha_i, \alpha_j) \, p_{j,k}(\alpha_j, \alpha_k) \, p_{k,i}(\alpha_k, \alpha_i) }{ p_i(\alpha_i) \, p_j(\alpha_j) \, p_k(\alpha_k) }
,
$$
which recovers the classical version of the superposition approximation\cite{kirkwood1935, born1946}.
\note{Eq.~(4.6) in \cite{born1946}.}



\subsection{Mutual information expansion (MIE)}


The MIE method uses the above framework to derive approximate expressions of the entropy.
%
Let us first define, in analogy to Eq.~\eqref{eq:S_W},
the marginal entropies and their corrections for subsets as
%
\begin{align*}
  S_I
  &= k_B \, \overline{ W_I(\alpha_I) }
  ,
  \\
  \delta S_J
  &= k_B \, \overline{ \delta W_J(\alpha_J) }
  ,
\end{align*}
%
Using Eq.~\eqref{eq:WI_dWJ_MIE} in Eq.~\eqref{eq:S_W},
we get an expression for the entropy for the $k$th-order MIE,
\begin{equation}
  S
  =
  \sum_{|J| \le k} \delta S_J
  =
  k_B \,
  \sum_{|J| \le k} \overline{ \delta W_J(\alpha_J) }
  ,
  \label{eq:ent_MIE1}
\end{equation}
where $J$ goes through all subsets of $\{1, \dots, n\}$ satisfying $|J| \le k$,
and $\delta W_J(\alpha_J)$ can be computed from Eq.~\eqref{eq:dWJ_WI}.

Conventionally, the sum in Eq.~\eqref{eq:ent_MIE1}
is arranged by the subset size as
\begin{equation}
  \frac{S}{k_B}
  =
  \sum_{j = 1}^k (-1)^{j-1} \MI_j
  ,
  \notag
  %\label{eq:S_MIE}
\end{equation}
%
where the $j$th-order mutual or interaction information, $\MI_j$, is defined as
%
\begin{align}
  \MI_j
  &\equiv (-1)^{j-1} \sum_{|J| = j}
  \delta S_J / k_B
  %\overline{ \delta W_J(\alpha_J) }
  %\notag \\
  %&=
  %\sum_{ |I| \le j }
  %(-1)^{|I| - 1}
  %{n - |I| \choose j - |I|}
  %\, \frac{S_I}{k_B}
  .
  \notag
  %\label{eq:MI_comb}
\end{align}
%$$
%\frac{ S_I } { k_B } \equiv \overline{ W_I(\alpha_I) }.
%$$
%
%Thus, Eq.~\eqref{eq:MI_comb} shows that
%the mutual information is a linear combination of
%the entropies of the subsets.

%Using Eq.~\eqref{eq:dWJ_WI}, we have for $j \le k$,
%\begin{align}
%  \MI_j
%  &=
%  \sum_{ |J| = j }
%  \sum_{ I \subseteq J }
%  (-1)^{|I| - 1}
%  \, \overline{ W_I(\alpha_I) }
%  \notag \\
%  &=
%  \sum_{ |I| \le j }
%  (-1)^{|I| - 1}
%  {n - |I| \choose j - |I|}
%  \, \overline{ W_I(\alpha_I) }
%  ,
%  \label{eq:MIj_expansion}
%\end{align}
%%
%where the appearance of the binomial coefficient
%is due to the number of ways of forming
%a size-$j$ superset of $I$,
%which is given by the number of ways of choosing
%$j - |I|$ DOFs out of the $n - |I|$ remaining DOFs.
%
%Using Eq.~\eqref{eq:MIj_expansion} in Eq.~\eqref{eq:S_MIE} yields
%%
%\begin{align}
%  \frac{S}{k_B}
%  &=
%  \sum_{j=1}^k
%  \sum_{|I| \le j}
%  (-1)^{j - |I|}
%  {n - |I| \choose j - |I|}
%  \, \overline{ W_I(\alpha_I) }
%  \notag\\
%  &=
%  \sum_{|I| \le k}
%  (-1)^{ k - |I| }
%  {n - |I| - 1 \choose k - |I|}
%  \, \overline{ W_I(\alpha_I) }
%  ,
%  \notag
%  %\label{eq:S_MIE_expansion}
%\end{align}
%%
%where we have used the combinatorial identity
%$$
%\sum_{r = 0}^R (-1)^r \, {m \choose r}
%=
%(-1)^R \, {m - 1 \choose R},
%$$
%where $0 \le R \le m$, and we have defined ${-1 \choose 0} \equiv 1$.
%\note{ See, for example, \url{https://en.wikipedia.org/wiki/Binomial\_coefficient\#Partial\_sums} }


From Eq.~\eqref{eq:dWJ_WI},
we can see that the mutual information, $\MI_j$, is just a linear combination
of the entropies, $S_I$'s,
of the subsets, $I$'s, of size no greater than $j$.
%
If we estimate $S_I$ using the direct counting method
by counting the occurrences in the subspace of $\alpha_I$ as
$$
\hat S_I = - k_B \sum_{\alpha_I} \hat p(\alpha_I) \, \ln \hat p(\alpha_I),
$$
the result, $\hat S_I$, would also suffer from the bias discussed
in Sec.~\ref{sec:fsbias}, and thus should be corrected accordingly.
%
As the subspace of $\alpha_I$ is the direct product of
the individual spaces of all elements in $I$,
its size, $M_I = \prod_{i \in I} M_i$
(with $M_i$ being the number of possibilities for the $i$th DOF),
grows exponentially with the size of $I$.
%
Thus, according to Eq.~\eqref{eq:Shat_ave},
the bias in uncorrected result would be much more noticeable
in a higher-order MIE.




\section{\label{sec:Potts}
Potts model
}

Here we give details of the Potts model used in our numerical tests.
%
This model consists of $n$ spins that spread evenly on a line.
%
Each spin, $s_i$, can assume one of the $q$ states with equal probability.
%
The Hamiltonian is given by
$$
H = -J \sum_{i = 1}^{n-1} s_i \, s_{i+1}.
$$

This model is analytically solvable. % because of its linear structure.
%
The partition function is given by
$Z_n = q \, (r + q)^{n-1}$,
where $r = e^{\beta \, J} - 1$.
%
The transition matrix between two neighboring sites is
$T_{i,i+1}(t_1, t_2) = r \, \delta_{t_1, t_2} + 1$.
%
The correlation between two spins $k$ sites apart
can be derived from the $k$th-order transition matrix,
which is the $k$th power of the above transition matrix
for nearest neighbors:
$$
T_{i,i+k}(t_1, t_2) = r^k \, \delta_{t_1, t_2}
+ \frac{ (r + q)^k - r^k } { q }.
$$
The joint distribution of two spins of $k$ sites apart, $i$ and $j = i+k$,
is then given by
\begin{equation}
p_{i, j}(t_1, t_2) = T_{i, j}(t_1, t_2) / Z_{k+1},
  \label{eq:Potts_pij}
\end{equation}
and the conditional probability of $s_j$ assuming the value of $t_2$
given that $s_i = t_1$ is
$$
p\left(s_j = t_2 \middle| s_i = t_1 \right) = \frac{ p_{i, j}(t_1, t_2) } { 1/q }.
$$
Because of the linear structure,
the joint distribution of $m$ sites, $i_1, i_2, \dots, i_m$
($i_1 < i_2 < \cdots < i_m$),
can be derived from a product of conditional probabilities
\begin{align}
  &p_{i_1, \dots, i_m}(t_1,\dots,t_m)\
  \notag\\
  &\qquad
  = p_{i_1, i_2}(t_1,t_2) \,
  p(s_{i_3} = t_3 | s_{i_2} = t_2)
  \notag\\
  &\qquad \qquad \quad \cdots
  p(s_{i_{m}} = t_{m} | s_{i_{m-1}} = t_{m-1})
  \notag\\
  &\qquad
  =
  q^{m-2} \, p_{i_1, i_2}(t_1,t_2) \cdots p_{i_{m-1}, i_m}(t_{m-1}, t_m)
  ,
  \label{eq:Potts_jointp}
\end{align}


From the above results, we can derive the exact entropy
and the values we expect to obtain from the MIE method.
%
First, for a pair of spins $k$ sites apart,
the entropic correction can be deduced from Eq.~\eqref{eq:Potts_pij} as
\begin{align}
  \delta S_{i, i+k}
  &=
  -k_B \left[
    (1 - q^{-1}) \, A_k \ln A_k
  + q^{-1} \, B_k \ln B_k \right]
  \notag\\
  &=
  -\frac{k_B}{2} (q-1) R^{2k}
  +\mathcal{O}(R^{3k})
 ,
\end{align}
where
$A_k = 1 - R^k$,
$B_k = 1 + (q-1) R^k$,
and $R = r/(r+q)$.
%
This result shows the pair correction is of the order of magnitude of $R^k$.
%
Next, from Eq.~\eqref{eq:Potts_jointp},
we have for $m$ sites,
$i_1$, $i_2$, \dots, $i_m$ ($i_1 < i_2 < \cdots < i_m$),
\begin{align}
S_{i_1, \dots, i_m}
  &= - k_B \, \overline{\ln p_{i_1,\dots, i_m}}
  \notag\\
  &= S_{i_1,i_2} + \cdots + S_{i_{m-1},i_m} - (m-2) \, k_B \, \ln q
  \notag\\
  &= \delta S_{i_1,i_2} + \cdots + \delta S_{i_{m-1},i_m} + m \, k_B \, \ln q
  .
  \label{eq:Potts_jointS}
\end{align}
%
By induction, we can show that
%
\begin{equation}
  \delta S_{i_1, i_2, \dots, i_m} = (-1)^m \delta S_{i_1, i_m}.
  \label{eq:Potts_jointdS}
\end{equation}
%
We can obtain the exact entropy of the system
by applying Eq.~\eqref{eq:Potts_jointS} to all sites:
\begin{align*}
  S =
  n \, k_B \, \ln q
  + \sum_{i=1}^{n-1} \delta S_{i,i+1}
  ,
\end{align*}
which is the sum of the marginal entropies of the $n$ sites
and the pair corrections of nearest neighbors.
%
This result, however, is different from that from the second-order MIE,
because the latter also includes pair corrections
of non-neighboring sites, i.e.,
$$
S^{(2)} = S + \sum_{i, j \ge i+2} \delta S_{i, j}
,
$$
which shows the error of second-order MIE is $\mathcal O(R^4)$.
By Eq.~\eqref{eq:Potts_jointdS}, we can further show that
for the $m$th order\footnote{
  For two spins $i$ and $j = i + k$ that are $k$ sites apart (assuming $k \ge m$),
  the contribution of $\delta S_{i, j}$ to the $m$th-order MIE
  can be computed as follows.
  %
  In the 2nd order, it is taken into account once for the pair $i$ and $j$.
  %
  In the 3rd order, it also contributes via the $k-1$ triplets, namely,
  $\{i, i+1, j\}, \dots, \{i, j-1, j\}$,
  and each with a negative sign [because $\delta S_{i,i_1,j} = -\delta S_{i,j}$,
  according to Eq.~\eqref{eq:Potts_jointdS}].
  %
  Generally, in the $m$th order,
  it contributes via the ${k-1 \choose m-2}$ unordered $m$-tuples
  with a sign of $(-1)^m$,
  where the binomial coefficient represents the number of ways
  of selecting the $m-2$ middle spins from the $k-1$ spins
  between spins $i$ and $j$.
  %
  The total contribution is thus
  $1 - {k-1 \choose 1} + {k-1 \choose 2} - \cdots + (-1)^{m}{k-1 \choose m-2}
  = (-1)^m {k-2 \choose m-2}$.
}
%
$$
S^{(m)} = S + \sum_{i, j \ge i+m} (-1)^m {j-i-2 \choose m-2} \, \delta S_{i,j}
,
$$
and thus the error of the $m$-order MIE is $\mathcal O(R^{2m})$.
%
This suggests that the MIE method would reach convergence
for this linear system when $R < 1$, or when the temperature is sufficiently high.
%
However, it can be more difficult to reach convergence along the order
for higher-dimensional or more complex systems.\cite{goethe2017}


\bibliography{simul}
\end{document}

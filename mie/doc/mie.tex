\documentclass[preprint, superscriptaddress]{revtex4-1}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{upgreek}
\usepackage[table,usenames,dvipsnames]{xcolor}
%\usepackage{tikz}
%\usetikzlibrary{calc}
\usepackage{hyperref}
\usepackage{setspace}

\setcitestyle{super}

\hypersetup{
  colorlinks,
  linkcolor={blue!80!black},
  citecolor={green!20!black},
  urlcolor={blue!80!black}
}


\definecolor{DarkBlue}{RGB}{0,0,64}
\definecolor{DarkBrown}{RGB}{64,20,10}
\definecolor{DarkGreen}{RGB}{0,64,0}
\definecolor{DarkPurple}{RGB}{128,0,84}
\definecolor{LightGray}{gray}{0.85}
% annotation macros
\newcommand{\repl}[2]{{\color{gray} [#1] }{\color{blue} #2}}
\newcommand{\add}[1]{{\color{blue} #1}}
\newcommand{\del}[1]{{\color{gray} [#1]}}
\newcommand{\note}[1]{{\color{DarkGreen}\footnotesize \textsc{Note.} #1}}

\newcommand{\MI}{\mathcal I} % mutual information

\begin{document}
\title{Bias of the configuration entropy estimated from counting-based methods}

\author{Justin A. Drake}
\affiliation{
Sealy Center for Structural Biology and Molecular Biophysics,
The University of Texas Medical Branch,
Galveston, Texas 77555-0304, USA}
\author{Danielle D. Seckfort}
\affiliation{
Quantitative and Computational Biosciences,
Baylor College of Medicine, Houston, Texas 77030, USA}
\author{Omneya Nassar}
\affiliation{
Sealy Center for Structural Biology and Molecular Biophysics,
The University of Texas Medical Branch,
Galveston, Texas 77555-0304, USA}
\author{B. Montgomery Pettitt}
\email{mpettitt@utmb.edu}
\affiliation{
Sealy Center for Structural Biology and Molecular Biophysics,
The University of Texas Medical Branch,
Galveston, Texas 77555-0304, USA}
\affiliation{
Quantitative and Computational Biosciences,
Baylor College of Medicine, Houston, Texas 77030, USA}

\begin{abstract}
We show that the configuration entropy estimated from methods based on counting occurrences
carries a negative bias
that is inversely proportional to the simulation length.
%
In particular, for the mutual information expansion method,
we show that the magnitude of the bias can grow rapidly with the order of the expansion.
\end{abstract}

\maketitle


\section{Introduction}

Entropy is a key concept of statistical mechanics that provides a quantitative measure of
the number of possible states a system can go through during its evolution.
%
In particular, the configuration entropy is defined,
for $M$ coarse-grained states in the configuration space, as
%
\begin{equation}
  S
  \equiv
  -k_B \sum_{\alpha = 1}^M p(\alpha) \, \ln p(\alpha)
  ,
  \label{eq:entropy_def}
\end{equation}
%
where $p(\alpha)$ is the probability of discrete state $\alpha = 1, \dots, M$,
and $k_B$ is the Boltzmann constant.
%
For the even distribution, $p(\alpha) \equiv 1/M$,
we can readily verify that $S \equiv k_B \, \ln M$
is proportional to the logarithm of the number of states.

We can readily apply Eq.~\eqref{eq:entropy_def} to a trajectory
obtained from a molecular dynamics (MD) or Monte Carlo (MC) simulation,
with the probabilities estimated from the observed occurrences of the states.
%
Among the occurrence-counting-based methods for estimating the configuration entropy
the method of mutual information expansion (MIE) is notable
for its potential to handle a vast state space of a complex system.
%
The MIE method applies to a many-body system
whose degrees of freedom are only loosely coupled.
%
Starting from the sum of marginal entropies of individual components,
the method offers a cascade of levels of corrections
to systematically absorb the contributions of higher-order correlations
of increasingly larger groups, such as pairs, triplets, quartets, etc.

However, a common drawback of the above counting-based methods
lies in its tendency to underestimate the configuration entropy for a finite sample
derived from a realistic simulation.
%
Thus, it is useful to understand the nature of this bias
and its dependence on the sample size.
%
This knowledge can hopefully help us derive a finite-size correction.

Here we wish to show that the configuration entropy derived
from the counting-based methods (including the MIE method)
generally carries a negative bias that scales inversely with the trajectory length.
%
The bias of the MIE method, in particular,
can grow dramatically with the order of tuples.
%
In some cases, we can reduce the magnitude of the bias
by imposing some restrictions on the higher-order tuples included.

\section{Method}

\subsection{\label{sec:fsbias}
Finite-size bias in the direct counting method}

We will first discuss the finite-size bias associated with
the direct application of Eq.~\eqref{eq:entropy_def},
which will be referred to as the direct counting method below.
%
Here, we estimate the probability $p(\alpha)$
by the frequency, $\hat{p}(\alpha) = \hat{n}(\alpha) / t$,
where $\hat{n}(\alpha)$ is the number of times state $\alpha$ occurring in the trajectory of $t$ steps,
and
%
\begin{equation}
  \hat S
  =
  -k_B \sum_{\alpha = 1}^M \hat{p}(\alpha) \, \ln \hat{p}(\alpha)
  .
  \label{eq:entropy_est}
\end{equation}
%
To better investigate the error of Eq.~\eqref{eq:entropy_est},
we will imagine an ensemble of trajectories of replica systems
that differ only by the random forces
and define the bias from the difference
between the ensemble average,
$\bigl\langle \hat S \bigr\rangle$
and the true value, $S$.
%
By the series expansion of $\hat{p}(\alpha)$ around $p(\alpha)$, we have
%
\begin{align*}
  \hat S
  =
  S
  &- k_B \sum_{\alpha = 1}^M
    \bigl[\ln p(\alpha) + 1 \bigr]
    \bigl[ \hat{p}(\alpha) - p(\alpha) \bigr]
  \\
  &- k_B \sum_{\alpha = 1}^M
    \frac{ \bigl[ \hat{p}(\alpha) - p(\alpha) \bigr]^2 } { 2 \, p(\alpha) }
  .
  %\label{eq:Shat_series}
\end{align*}
%
\note{The series expansion of the function, $h(x) = -x \, \ln x$,
around $x_0$,
%
\begin{align*}
  h(x)
  &\approx h(x_0) + h'(x_0) (x - x_0) + \frac{h''(x_0)}{2} (x - x_0)^2 \\
  &\approx h(x_0) - (\ln x_0 + 1) ( x - x_0) - \frac{1}{2 \, x_0} (x - x_0)^2
  .
\end{align*}
}
%
Assuming the sampling is unbiased,
we have $\langle \hat{p}(\alpha) \rangle = p(\alpha)$.
%
Thus, after ensemble averaging, we get
%
\begin{align}
  \bigl\langle \hat S \bigr\rangle
  \approx
  S - k_B \sum_{\alpha = 1}^M
    \frac{ \operatorname{var} \hat{p}(\alpha) } { 2 \, p(\alpha) }
  =
  S - k_B \sum_{\alpha = 1}^M
    \frac{ \operatorname{var} \hat{n}(\alpha) } { 2 \, p(\alpha) \, t^2 }
  .
  \label{eq:entest_2nd}
\end{align}

By definition, we have
%
\begin{equation*}
  \hat{n}(\alpha) = \sum_{t' = 1}^t \delta_{\alpha, \alpha(t')},
\end{equation*}
%
where $\alpha(t')$ denotes the state at step $t'$,
and $\delta_{\alpha, \gamma}$ is the Kronecker delta,
which takes the value of $1$ if $\alpha = \gamma$, or $0$ otherwise.
%
We can readily show that
\begin{align*}
  \bigl\langle \hat{n}(\alpha) \bigr\rangle
  &=
  t \, p(\alpha), \\
  %
  \operatorname{var}{\hat{n}(\alpha)}
  &=
  \left\langle \hat{n}(\alpha)^2 \right\rangle - \langle \hat{n}(\alpha) \rangle^2
  \\
  &=
  t \, p(\alpha) \, \bigl( 1 - p(\alpha) \bigr) \, \bigl(2 \, \tau_\alpha + 1\bigr)
  ,
\end{align*}
%
where $\tau_\alpha$ is the integral autocorrelation time of $\delta_{\alpha, \alpha(t)}$,
defined as $\tau_\alpha = \sum_{t = 1}^\infty \kappa_\alpha(t)/\kappa_\alpha(0)$,
with the autocorrelation function given by
$$
\kappa_\alpha(t) = \bigl(\delta_{\alpha, \alpha(t)} - p(\alpha)\bigr)
\bigl(\delta_{\alpha, \alpha(0)} - p(\alpha)\bigr).
$$
%
Thus, we can rewrite Eq.~\eqref{eq:entest_2nd} as
%
\begin{align}
  \bigl\langle \hat S \bigr\rangle
  &\approx
  S - k_B \sum_{\alpha = 1}^M
    \frac{ \bigl(1 - p(\alpha)\bigr) ( 2 \, \tau_\alpha + 1) } { 2 \, t }
  \notag\\
  &\approx
  S - k_B
    \frac{ (M - 1) ( 2 \, \tau + 1) } { 2 \, t }
  .
  \label{eq:Shat_ave}
\end{align}
where in the second step,
we have assumed that all integral autocorrelation times are the same.

This result shows that the entropy estimated
from direct counting
is expected to carry a negative bias
that is inversely proportional to the trajectory length.
%
As we will see, the same type of bias also exists
in the more elaborate the MIE method.


\subsection{Kirkwood superposition approximation}


The direct counting method breaks down
for a complex system of many degrees of freedom (DOFs),
because as the number of states increases exponentially with the number of DOFs,
the rapidly diminishing number of samples collected in each state
can readily deteriorate the precision of the entropy estimate.
%
For these many-body systems,
we may alternatively adopt
the mutual information expansion (MIE) method,
which is a more advanced, albeit approximate, method designed for a system
with many largely-independent DOFs.
%
The MIE method is based on the idea of
the Kirkwood superposition approximation (KAS)\cite{kirkwood1935, born1946},
or that of interaction information\cite{mcgill1954},
which we will briefly review below.

We first recall that the (dimensionless) potential of mean force (PMF)
is defined for a distribution, $p(\alpha)$,
as
$$
W(\alpha) = -\ln p(\alpha) = \ln\left[ \frac{1}{p(\alpha)} \right].
$$
The PMF can be interpreted as
the free energy of confining the ensemble to a single state,
or the amount of work
(in the unit of $k_B \, T$'s, with $T$ being the temperature)
required to transfer an average system randomly drawn from the ensemble (of probability $1$)
to state $\alpha$ [of probability $p(\alpha)$].
%
We can now rewrite Eq.~\eqref{eq:entropy_def} as
%
\begin{equation}
  \frac{S}{k_B}
  =
  \sum_{\alpha = 1}^M p(\alpha) \, W(\alpha)
  =
  \overline{ W(\alpha) }
  ,
  \label{eq:S_W}
\end{equation}
%
which shows that the configuration entropy
is the mean PMF.
%
The superposition approximation is based on the intuition
that for a physical or extensive system composed of multiple DOFs,
the PMF of the entire system is roughly decomposable to
the sum of the marginal PMFs of individual DOFs.

Consider a many-body system of $n$ DOFs.
%
We label the DOFs numerically, $1, \dots, n$,
and specify the state as $\alpha = (\alpha_1, \dots, \alpha_n)$.
%
%For simplicity, we will assume that the degrees of freedom are identical.
%
For a subset of DOFs, $I = \{i, j, \dots\}$,
we denote the joint distribution by $p_I(\alpha_I)$,
and define the joint PMF as
%
\begin{equation}
  W_I(\alpha_I) = -\ln p_I(\alpha_I)
  .
  \label{eq:WI_def}
\end{equation}
%
For example,
the marginal PMF of a single DOF $i$ is given by $W_i$ (omitting the argument),
the joint PMF of a pair $\{i, j\}$ by $W_{i, j}$,
that of a triplet $\{i, j, k\}$ by $W_{i, j, k}$, etc.

Now by assuming that the DOFs are largely independent,
we can write the PMF of a pair $\{i, j\}$
as the sum of the marginal PMFs of the two individual DOFs $i$ and $j$
and a hopefully small two-body correction, $\delta W_{i,j}$,
%
\begin{equation}
  W_{i,j}(\alpha_i, \alpha_j)
  =
  W_i(\alpha_i) + W_j(\alpha_j)
  + \delta W_{i,j}(\alpha_i, \alpha_j)
  .
  \label{eq:W_ij}
\end{equation}
%
Similarly, for a triplet, $\{i, j, k\}$,
we can define the three-body correction, $\delta W_{i,j,k}$, via
%
\begin{align}
  &W_{i,j,k}(\alpha_i, \alpha_j, \alpha_k)
  =
  W_i(\alpha_i) + W_j(\alpha_j) + W_k(\alpha_k)
  \notag \\
  &\quad
  + \delta W_{i,j}(\alpha_i, \alpha_j)
  + \delta W_{j,k}(\alpha_j, \alpha_k)
  + \delta W_{k,i}(\alpha_k, \alpha_i)
  \notag \\
  &\quad
  + \delta W_{i,j,k}(\alpha_i, \alpha_j, \alpha_k)
  .
  \label{eq:W_ijk}
\end{align}
%


Generally, we can define the PMF correction, $\delta W_I$,
for an arbitrary set of DOFs, $I$,
implicitly through the recurrence relation,
%
\begin{equation}
  W_I(\alpha_I)
  =
  \sum_{J \subseteq I}
  \delta W_J(\alpha_J)
  ,
  \label{eq:WI_dWJ}
\end{equation}
%
where the sum goes through all subsets $J$'s of $I$,
and we have defined
$\delta W_\emptyset \equiv W_\emptyset \equiv 0$.
%
Note that in this definition,
$\delta W_i \equiv W_i$
for a single DOF $i$.
%
We can readily verify that
Eqs.~\eqref{eq:W_ij} and \eqref{eq:W_ijk}
are special cases of Eq.~\eqref{eq:WI_dWJ}

Since Eq.~\eqref{eq:WI_dWJ} is a linear relation,
we can invert it to get an explicit definition of $\delta W_J$
through the inclusion-exclusion principle, or the inverse M\"{o}bius transform\cite{bjorklund2007},
%
\begin{equation}
  \delta W_J(\alpha_J)
  =
  \sum_{I \subseteq J}
  (-1)^{|J| - |I|}
  W_I(\alpha_I)
  ,
  \label{eq:dWJ_WI}
\end{equation}
%
where $|I|$ denotes the size of subset $I$.
%
Below we will refer to the subset sizes, $|I|$ and $|J|$,
as the orders of $W_I(\alpha_I)$ and $\delta W_J(\alpha_J)$.
%
For example, we have, for a pair, $\{i, j\}$,
%
\begin{equation}
  \delta W_{i,j}(\alpha_i, \alpha_j)
  =
  W_{i,j}(\alpha_i, \alpha_j)
  - W_{i}(\alpha_i) - W_{j}(\alpha_j)
  ,
  \notag
  %\label{eq:dWij}
\end{equation}
%
or, for a triplet, $\{i, j, k\}$,
%
\begin{align*}
  &\delta W_{i,j,k}(\alpha_i, \alpha_j, \alpha_k)
  =
  W_{i,j,k}(\alpha_i, \alpha_j, \alpha_k)
  \notag\\
  &\quad
  - W_{i,j}(\alpha_i, \alpha_j)
  - W_{j,k}(\alpha_j, \alpha_k)
  - W_{k,i}(\alpha_k, \alpha_i)
  \notag\\
  &\quad
  + W_i(\alpha_i)
  + W_j(\alpha_j)
  + W_k(\alpha_k)
  .
  %\label{eq:dWijk}
\end{align*}

\note{
We can readily verify Eq.~\eqref{eq:dWJ_WI}
by a direct expansion using Eq.~\eqref{eq:WI_dWJ}.
%
Consider a particular subset $K \subseteq J$.
Let us compute the contribution of $\delta W_K$
to the right-hand side of Eq.~\eqref{eq:dWJ_WI}.
%
Note that $\delta W_K$ contributes only to those terms
whose $I$ is a superset of $K$,
and the contribution coefficient is $(-1)^{|J|-|I|}$.
%
If $K$ is strict subset of $J$,
then there is at least one component $j^*$ that belongs to $J$
but not to $K$,
and all supersets, $I$'s,
satisfying the above condition, and that given by Eq.~\eqref{eq:dWJ_WI},
$K \subseteq I \subseteq J$,
can be partitioned to two equally populated groups,
according to whether the $I$ has $j^*$ or not.
%
However, because the corresponding members of the two groups
contribute the same value with opposite signs to the sum,
the total contribution is zero.
%
This means $\delta W_K$ can contribute to the right-hand side
if and only if $K = J$, and the contribution is $1 \cdot W_J$,
as intended.
}

%If we limit ourselves to supersets, $I$'s, of size $i$,
%then there are
%${|J| - |K| \choose i - |K|}$ ways
%of including components other than those in $K$ into $I$,
%and the total contribution is
%$(-1)^{|J|-i} {|J| - |K| \choose i - |K|}$.
%%
%Summing over the possible sizes yields
%$$
%\sum_{j=|K|}^{|J|} (-1)^{|J|-i} {|J| - |K| \choose i - |K|}
%= (1 - 1)^{|J| - |K|} = \delta_{|J|, |K|}.
%$$
%This shows that the total contribution of $\delta W_K$
%to the right-hand side of Eq.~\eqref{eq:dWJ_WI}
%vanishes unless $K = J$,
%which is the intended result.

\note{
Just as we define the PMFs, $W_I$'s, from the joint distributions
through Eq.~\eqref{eq:WI_def},
we can associate the corrections, $\delta W_I$'s,
with the (excess) correlation functions, $g_I$'s, as
%
\begin{equation}
  g_I(\alpha_I) \equiv e^{-\delta W_I(\alpha_I)}
  .
  \notag
  %\label{eq:gI_def}
\end{equation}
%
This definition is consistent with the usual definition
of the pair correlation function (as in liquid state theory\cite{hansen}),
$g_{i,j}(\alpha_i, \alpha_j)$ for a pair, $\{i, j\}$,
\begin{equation}
  g_{i,j}(\alpha_i, \alpha_j)
  =
  \frac{ p_{i,j}(\alpha_i, \alpha_j) } { p_i(\alpha_i) \, p_j(\alpha_j) }
  .
  \notag
  %\label{eq:gij_MIE2}
\end{equation}
Similarly, for a triplet, $\{i, j, k\}$, we have
$$
g_{i,j,k}(\alpha_i, \alpha_j, \alpha_k)
=
\frac{ p_{i,j,k}(\alpha_i, \alpha_j \, \alpha_k) \, p_i(\alpha_i) \, p_j(\alpha_j) \, p_k(\alpha_k) }{ p_{i,j}(\alpha_i, \alpha_j) \, p_{j,k}(\alpha_j, \alpha_k) \, p_{k,i}(\alpha_k, \alpha_i) }
.
$$
}



Without a priori assumption on the magnitudes of the corrections,
Eqs.~\eqref{eq:WI_dWJ} and \eqref{eq:dWJ_WI} are exact relations
that serve only as the mathematical definition of $\delta W_I$.
%
However, its practical value lies in that in many cases
with largely independent DOFs,
we can often assume
that the corrections of larger subsets are negligible
so that a high-order PMF of many DOFs can be approximately constructed
from a linear superposition of a few low-order ones.


If we can assume the corrections, $\delta W_J$'s,
for orders greater than $k$ are negligible,
%
\begin{equation}
  \delta W_J(\alpha_J) = 0
  \qquad
  \mbox{for $|J| \ge k$}
  ,
  \notag
  %\label{eq:mie_trunc}
\end{equation}
%
we can then approximate Eq.~\eqref{eq:entest_2nd} as
%
\begin{equation}
  W_I(\alpha_I)
  =
  \sum_{ \substack{J \subseteq I, \; |J| \le k} }
  \!\! \delta W_J(\alpha_J)
  ,
  \label{eq:WI_dWJ_MIE}
\end{equation}
%
which can be inverted as
%
\begin{equation}
  \delta W_J(\alpha_J)
  =
  \begin{dcases}
    \sum_{ I \subseteq J }
    (-1)^{|J| - |I|} W_I(\alpha_I)
    &\mbox{for $|J| \le k$}
    ,
    \\
    0
    &\mbox{for $|J| > k$}
    .
  \end{dcases}
  \label{eq:dWJ_WI_MIE}
\end{equation}
%

In this way, we now only require the lower-order $W_I(\alpha_I)$'s
for subsets satisfying $|I| \le k$
as the independent variables,
which are to be estimated from simulation trajectories,
because the higher-order $W_I(\alpha_I)$'s for larger subsets
can be readily constructed from Eq.~\eqref{eq:WI_dWJ_MIE}
using the values of $\delta W_J(\alpha_J)$'s
given by Eq.~\eqref{eq:dWJ_WI_MIE}.

For example,
in the second-order approximation,
we assume that
$\delta W_{i, j, k} = \delta W_{i,j,k, l} = \cdots = 0$.
%
Then,
%$\delta W_{i, j} = W_{i,j} - W_i - W_j$,
$$\delta W_{i, j}(\alpha_i, \alpha_j)
= W_{i,j}(\alpha_i, \alpha_j) - W_i(\alpha_i) - W_j(\alpha_j),$$
and the triplet PMF is given as a superposition
of the pair and marginal PMFs,
%$$
%W_{i, j, k} = W_{i,j} + W_{j,k} + W_{k,i} - W_i - W_j - W_k.
%$$
\begin{align*}
W_{i,j,k}(\alpha_i, \alpha_j, \alpha_k)
=
W_{i,j}(\alpha_i, \alpha_j) + W_{j,k}(\alpha_j, \alpha_k)\\
  + W_{k,i}(\alpha_k, \alpha_i)
  - W_i(\alpha_i) - W_j(\alpha_j) - W_k(\alpha_k)
.
\end{align*}
%
The latter, written in terms of the distributions, $p_I(\alpha_I)$'s,
[cf. Eq.~\eqref{eq:WI_def}]
%
$$
p_{i,j,k}(\alpha_i, \alpha_j, \alpha_k)
=
\frac{ p_{i,j}(\alpha_i, \alpha_j) \, p_{j,k}(\alpha_j, \alpha_k) \, p_{k,i}(\alpha_k, \alpha_i) }{ p_i(\alpha_i) \, p_j(\alpha_j) \, p_k(\alpha_k) }
,
$$
recovers the classical version of the superposition approximation\cite{kirkwood1935, born1946}.
\note{Eq. (4.6) in \cite{born1946}.}

\subsection{Mutual information expansion (MIE)}


Using Eq.~\eqref{eq:WI_dWJ_MIE} in Eq.~\eqref{eq:S_W},
we get an expression for the entropy for the $k$th-order MIE,
\begin{equation}
  \frac{S}{k_B}
  =
  \sum_{|J| \le k} \overline{ \delta W_J(\alpha_J) },
  \label{eq:ent_MIE1}
\end{equation}
where $J$ goes through all subsets of $\{1, \dots, n\}$ satisfying $|J| \le k$,
and $\delta W_J(\alpha_J)$ can be computed from Eq.~\eqref{eq:dWJ_WI}.

Conventionally, the sum in Eq. \eqref{eq:ent_MIE1}
is arranged by the subset size as
\begin{equation}
  \frac{S}{k_B}
  =
  \sum_{j = 1}^k (-1)^{j-1} \MI_j
  ,
  \label{eq:S_MIE}
\end{equation}
%
where the $j$th-order mutual information, $\MI_j$, is defined as
$$
\MI_j \equiv (-1)^{j-1} \sum_{|J| = j} \overline{ \delta W_J(\alpha_J) }.
$$

Using Eq. \eqref{eq:dWJ_WI_MIE}, we have for $j \le k$,
\begin{align}
  \MI_j
  &=
  \sum_{ |J| = j }
  \sum_{ I \subseteq J }
  (-1)^{|I| - 1}
  \, \overline{ W_I(\alpha_I) }
  \notag \\
  &=
  \sum_{ |I| \le j }
  (-1)^{|I| - 1}
  {n - |I| \choose j - |I|}
  \, \overline{ W_I(\alpha_I) }
  ,
  \label{eq:MIj_expansion}
\end{align}
%
where the appearance of the binomial coefficient
is due to the number of ways of forming
a size-$j$ superset of $I$,
which is given by the number of ways of choosing
$j - |I|$ DOFs out of the $n - |I|$ remaining DOFs.

Using Eq.~\eqref{eq:MIj_expansion} in Eq.~\eqref{eq:S_MIE} yields
%
\begin{align}
  \frac{S}{k_B}
  &=
  \sum_{j=1}^k
  \sum_{|I| \le j}
  (-1)^{j - |I|}
  {n - |I| \choose j - |I|}
  \, \overline{ W_I(\alpha_I) }
  \notag\\
  &=
  \sum_{|I| \le k}
  (-1)^{ k - |I| }
  {n - |I| - 1 \choose k - |I|}
  \, \overline{ W_I(\alpha_I) }
  ,
  \notag
  %\label{eq:S_MIE_expansion}
\end{align}
%
where we have used the combinatorial identity
$$
\sum_{r = 0}^R (-1)^r \, {m \choose r}
=
(-1)^R \, {m - 1 \choose R}.
$$
\note{ See, for example, \url{https://en.wikipedia.org/wiki/Binomial\_coefficient\#Partial\_sums} }

In analogous to Eq.~\eqref{eq:S_W}, we can define
the marginal entropy of the subset $I$ as
$$
\frac{ S_I } { k_B } = \overline{ W_I(\alpha_I) }.
$$


If we estimate $S_I$ using the direct counting method,
by counting the occurrences in the subspace of $\alpha_I$ as
$$
\hat S_I = - k_B \sum_{\alpha_I} \hat p(\alpha_I) \, \ln p(\alpha_I),
$$
the result, $\hat S_I$, would also suffer from the bias discussed
in Sec.~\ref{sec:fsbias}, and thus should be corrected accordingly.
%
As the subspace of $\alpha_I$ is the direct product of
the individual spaces of all elements in $I$,
its size, $M_I = \prod_{i \in I} M_i$, grows exponentially with the size of $I$.
%
Thus, according to Eq. \eqref{eq:Shat_ave},
the bias in uncorrected result would be much more noticeable
in a higher-order version of MIE.


\bibliography{simul}
\end{document}

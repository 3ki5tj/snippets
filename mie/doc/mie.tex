\documentclass[preprint, superscriptaddress]{revtex4-1}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{upgreek}
\usepackage[table,usenames,dvipsnames]{xcolor}
%\usepackage{tikz}
%\usetikzlibrary{calc}
\usepackage{hyperref}
\usepackage{setspace}

\setcitestyle{super}

\hypersetup{
  colorlinks,
  linkcolor={blue!80!black},
  citecolor={green!20!black},
  urlcolor={blue!80!black}
}


\definecolor{DarkBlue}{RGB}{0,0,64}
\definecolor{DarkBrown}{RGB}{64,20,10}
\definecolor{DarkGreen}{RGB}{0,64,0}
\definecolor{DarkPurple}{RGB}{128,0,84}
\definecolor{LightGray}{gray}{0.85}
% annotation macros
\newcommand{\repl}[2]{{\color{gray} [#1] }{\color{blue} #2}}
\newcommand{\add}[1]{{\color{blue} #1}}
\newcommand{\del}[1]{{\color{gray} [#1]}}
\newcommand{\note}[1]{{\color{DarkGreen}\footnotesize \textsc{Note.} #1}}

\begin{document}
\title{Bias of the configuration entropy estimated from counting methods}

\author{Justin A. Drake}
\affiliation{
Sealy Center for Structural Biology and Molecular Biophysics,
The University of Texas Medical Branch,
Galveston, Texas 77555-0304, USA}
\author{Danielle D. Seckfort}
\affiliation{
Quantitative and Computational Biosciences,
Baylor College of Medicine, Houston, Texas 77030, USA}
\author{B. Montgomery Pettitt}
\email{mpettitt@utmb.edu}
\affiliation{
Sealy Center for Structural Biology and Molecular Biophysics,
The University of Texas Medical Branch,
Galveston, Texas 77555-0304, USA}
\affiliation{
Quantitative and Computational Biosciences,
Baylor College of Medicine, Houston, Texas 77030, USA}

\begin{abstract}
We show that the configuration entropy estimated from methods based on counting occurrences
carries a negative bias
that is inversely proportional to the simulation length.
%
In particular, for the method based on the mutual information expansion,
we show that the magnitude of the bias can grow rapidly with the order of the expansion.
\end{abstract}

\maketitle


\section{Introduction}

Entropy is a key concept of statistical mechanics that provides a quantitative measure of
the number of possible states a system can go through during its evolution.
%
In particular, the configuration entropy is defined,
for $M$ coarse-grained states in the configuration space, as
%
\begin{equation}
  S
  \equiv
  -k_B \sum_{\alpha = 1}^M p(\alpha) \, \ln p(\alpha)
  ,
  \label{eq:entropy_def}
\end{equation}
%
where $p(\alpha)$ is the probability of discrete state $\alpha = 1, \dots, M$,
and $k_B$ is the Boltzmann constant.
%
For the even distribution, $p(\alpha) \equiv 1/M$,
we can readily verify that $S \equiv k_B \, \ln M$
is proportional to the logarithm of the number of states.

We can readily apply Eq. \eqref{eq:entropy_def} to a trajectory
obtained from a molecular dynamics (MD) or Monte Carlo (MC) simulation,
with the probabilities estimated from the observed occurrences of the states.
%
Among the many methods for estimation of the configuration entropy based on this principle
the method of mutual information expansion (MIE) is a notable example
that can potentially handle a vast state space of a complex system.
%
The MIE method applies to a many-body system
 whose state space is decomposable to a direct product of individual component spaces.
%
Starting from the sum of the entropies from the component spaces,
the method offers a cascade of levels of corrections
that gradually take into account higher-order correlations
of pairs, triples, quadruples, etc.

However, a common drawback of the above counting-based methods is that
they tend to underestimate the configuration entropy for a finite sample
derived from a realistic simulation.
%
Thus, it is useful to understand the variance of this estimation bias with the sample size.
%
This knowledge can hopefully help derive a finite-size correction
through extrapolation along the system size.

Here we wish to show that the configuration entropy derived
from the counting-based methods (including MIE)
generally carries a negative bias that scales inversely with the trajectory length.
%
The bias of the MIE method, in particular,
can grow dramatically with the order of tuples.
%
We can reduce the magnitude of the bias
by imposing some restrictions to the higher-order tuples included.

\section{Method}

\subsection{Finite-size bias in the direct counting method}

We will first discuss the finite-size bias associated with
the direct application of Eq. \eqref{eq:entropy_def},
which will be referred to as the direct counting method below.
%
Here, we estimate the probability $p(\alpha)$
by the frequency, $f(\alpha) = n(\alpha) / t$,
where $n(\alpha)$ is the number of times state $\alpha$ occurring in the trajectory of $t$ steps,
and
%
\begin{equation}
  \hat S
  =
  -k_B \sum_{\alpha = 1}^M f(\alpha) \, \ln f(\alpha)
  .
  \label{eq:entropy_est}
\end{equation}
%
To facilitate the discussion on error,
we will imagine an ensemble of trajectories of replica systems
under the same condition but different random forces
and define the bias from the difference
between the ensemble average,
$\langle \hat S \rangle$
and the true value, $S$.
%
By the series expansion of $f(\alpha)$ around $p(\alpha)$, we have
%
\begin{align*}
  \hat S
  =
  S
  &- k_B \sum_{\alpha = 1}^M
    \bigl[\ln p(\alpha) + 1 \bigr]
    \bigl[ f(\alpha) - p(\alpha) \bigr]
  \\
  &- k_B \sum_{\alpha = 1}^M
    \frac{ \bigl[f(\alpha) - p(\alpha)\bigr]^2 } { 2 \, p(\alpha) }
  .
  %\label{eq:Shat_series}
\end{align*}
%
\note{The series expansion of the function, $h(x) = -x \, \ln x$,
around $x_0$,
%
\begin{align*}
  h(x)
  &\approx h(x_0) + h'(x_0) (x - x_0) + \frac{h''(x_0)}{2} (x - x_0)^2 \\
  &\approx h(x_0) - (\ln x_0 + 1) ( x - x_0) - \frac{1}{2 \, x_0} (x - x_0)^2
  .
\end{align*}
}
%
Assuming the sampling is unbiased,
we have $\langle f(\alpha) \rangle = p(\alpha)$.
%
Thus, after ensemble averaging, we get
%
\begin{align*}
  \langle \hat S \rangle
  \approx
  S - k_B \sum_{\alpha = 1}^M
    \frac{ \operatorname{var} f(\alpha) } { 2 \, p(\alpha) }
  =
  S - k_B \sum_{\alpha = 1}^M
    \frac{ \operatorname{var} n(\alpha) } { 2 \, p(\alpha) \, t^2 }
  .
  %\label{eq:Shat_series}
\end{align*}

By definition, we have
$$n(\alpha) = \sum_{t' = 1}^t \delta_{\alpha, \alpha(t')},$$
where $\alpha(t')$ denotes the state at step $t'$,
and $\delta_{\alpha, \gamma}$ is the Kronecker delta,
which takes the value of $1$ if $\alpha = \gamma$, or $0$ otherwise.
%
We can readily show that
\begin{align*}
  \langle n(\alpha) \rangle
  &=
  t \, p(\alpha), \\
  \operatorname{var}{n(\alpha)}
  &=
  \left\langle n(\alpha)^2 \right\rangle - \langle n(\alpha) \rangle^2
  \\
  &=
  t \, p(\alpha) \, \bigl( 1 - p(\alpha) \bigr) \, \bigl(2 \, \tau_\alpha + 1\bigr)
  ,
\end{align*}
%
where $\tau_\alpha$ is the integral autocorrelation time of $\delta_{\alpha, \alpha(t)}$,
defined as $\tau_\alpha = \sum_{t = 1}^\infty \kappa_\alpha(t)/\kappa_\alpha(0)$,
with the autocorrelation function given by
$$
\kappa_\alpha(t) = \bigl(\delta_{\alpha, \alpha(t)} - p(\alpha)\bigr)
\bigl(\delta_{\alpha, \alpha(0)} - p(\alpha)\bigr).
$$

\begin{align*}
  \langle \hat S \rangle
  &\approx
  S - k_B \sum_{\alpha = 1}^M
    \frac{ \bigl(1 - p(\alpha)\bigr) ( 2 \, \tau_\alpha + 1) } { 2 \, t }
  \\
  &\approx
  S - k_B
    \frac{ (M - 1) ( 2 \, \tau + 1) } { 2 \, t }
  .
  %\label{eq:Shat_series}
\end{align*}
where in the second step,
we have assumed that all integral autocorrelation time are the same.

This result shows that the entropy estimated
from the direct counting method
is expected to carry a negative bias
that is inversely proportional to the trajectory length.
%
As we will see, the same type of bias also exists
in more elaborate entropy-estimation methods
such as the MIE method.


\subsection{Kirkwood superposition approximation}


The direct counting method breaks down
for a complex system of many states,
because as the number of states increases,
the diminishing number of samples collected in each state
can readily deteriorate the precision of the entropy estimate.
%
For these many-body systems,
we may alternatively adopt
the mutual information expansion (MIE) method,
which is an approximate method designed for a system
with many largely-independent degrees of freedom.
%
The MIE method is based on the idea of
the Kirkwood superposition approximation (KAS)\cite{kirkwood1935},
or that of interaction information\cite{mcgill1954},
which we will briefly review below.

We first recall that the (dimensionless) potential of mean force (PMF)
is defined for a distribution, $p(\alpha)$,
as
$$
W(\alpha) = \ln\left[ \frac{1}{p(\alpha)} \right].
$$
The PMF can be interpreted as the amount of work
(in the unit of $k_B \, T$, with $T$ being the temperature)
required to commit an average system randomly drawn from ensemble of unknown state (of probability $1$)
to state $\alpha$ (of probability $p(\alpha)$).
%
We will refer to this work as the work of ensemble restriction (WES) below.
%
We can now rewrite Eq. \eqref{eq:entropy_def} as
%
\begin{equation}
  S =
  \sum_{\alpha = 1}^M p(\alpha) \, W(\alpha)
  =
  \overline{ W(\alpha) }
  ,
\end{equation}
%
which shows that the configuration entropy
is the mean WES.
%
For a thermodynamic system,
we might expect the WES to be decomposable to
the work of individual degrees of freedom plus some correlation.
%
The Kirkwood superposition approximation is based on this intuition.

Consider a many-body system with many component degrees of freedom.
%
We will label the degrees of freedom by $i$, $j$, \dots,
and for a state $\alpha$,
we will denote the corresponding components by
$\alpha_i$, $\alpha_j$, \dots.
%
%For simplicity, we will assume that the degrees of freedom are identical.
%
For a set of degrees of freedom, $I = \{i, j, \dots\}$,
we denote the marginal joint distribution by $p_I(\alpha_I)$,
and define the joint PMF as $W_I(\alpha_I) = -\ln p_I(\alpha_I)$.
%
For example,
the marginal PMF of a single degree of freedom $i$ is given by $W_i$ (omitting the argument),
that of a pair $\{i, j\}$ by $W_{i, j}$,
that of a triplet $\{i, j, k\}$ by $W_{i, j, k}$, etc.

Now by assuming that the components are largely independent,
we would expect that the PMF of a pair $\{i, j\}$
is the sum of the PMFs of the two individual components $i$ and $j$
and a two-body correction, $\delta W_{i,j}$,
%
\begin{equation}
  W_{i,j}(\alpha_i, \alpha_j)
  =
  W_i(\alpha_i) + W_j(\alpha_j)
  + \delta W_{i,j}(\alpha_i, \alpha_j)
  .
  \label{eq:W_ij}
\end{equation}
%
Similarly, for a triplet, $\{i, j, k\}$,
we can define the correction, $\delta W_{i,j,k}$ via
%
\begin{align}
  &W_{i,j,k}(\alpha_i, \alpha_j, \alpha_k)
  =
  W_i(\alpha_i) + W_j(\alpha_j) + W_k(\alpha_k)
  \notag \\
  &\quad
  + \delta W_{i,j}(\alpha_i, \alpha_j)
  + \delta W_{j,k}(\alpha_j, \alpha_k)
  + \delta W_{k,i}(\alpha_k, \alpha_i)
  \notag \\
  &\quad
  + \delta W_{i,j,k}(\alpha_i, \alpha_j, \alpha_k)
  .
  \label{eq:W_ijk}
\end{align}
%


Generally, we can define the PMF correction, $\delta W_I$,
for an arbitrary set of components, $I$,
implicitly through the recurrence relation,
%
\begin{equation}
  W_I(\alpha_I)
  =
  \sum_{J \subseteq I}
  \delta W_J(\alpha_J)
  ,
  \label{eq:WI_dWJ}
\end{equation}
%
where the sum goes through all subsets $J$'s of $I$,
and we have defined
$\delta W_\emptyset \equiv W_\emptyset \equiv 0$.
%
Note that in this definition,
$\delta W_i \equiv W_i$
for a single component $i$.
%
We can readily verify that
Eqs. \eqref{eq:W_ij} and \eqref{eq:W_ijk}
are special cases of Eq. \eqref{eq:WI_dWJ}

Without a priori assumption on the magnitudes of the corrections,
Eq. \eqref{eq:WI_dWJ} is an exact relation
that serves only as the mathematical definition of $\delta W_I$.
%
However, its practical value lies in that in many cases
with nearly independent components,
we can often assume, approximately,
that the corrections of larger subsets are negligible
so that a many-component PMF can be constructed
from a few low-order ones.

Since Eq. \eqref{eq:WI_dWJ} is a linear relation,
we can invert it to get an explicit definition of $\delta W_I$
through the inverse M\"{o}bius transform\cite{bjorklund2007},
\begin{equation}
  \delta W_I =
  \sum_{J \subseteq I}
  (-1)^{|I| - |J|}
  W_J
  ,
  \label{eq:dWI_WJ}
\end{equation}
%
where $|I|$ denote the size of set $I$.

We can readily verify Eq. \eqref{eq:dWI_WJ}
by a direct expansion using Eq. \eqref{eq:WI_dWJ}.
%
Consider a particular subset $K \subseteq I$.
Let us compute the contribution of $\delta W_K$
to the right-hand side of Eq. \eqref{eq:dWI_WJ}.
%
Note that $\delta W_K$ contributes only to those terms
whose $J$ is a superset of $K$,
and the contribution coefficient is $(-1)^{|I|-|J|}$.
%
If $K$ is strict subset of $I$,
then there is at least one component $i^*$ that belongs to $I$
but not to $K$,
and all supersets, $J$'s,
satisfying the condition given by Eq. \eqref{eq:dWI_WJ},
$K \subseteq J \subseteq I$,
can be partitioned to two equally populated groups,
according to whether the $J$ has $i^*$ or not.
%
However, because the corresponding members of the two groups
contribute opposite signs to the sum,
the total contribution is zero.
%
This means $\delta W_K$ can contribute to the right-hand side
if and only if $K = I$, and the contribution is $1 \cdot W_I$,
as intended.


%If we limit ourselves to supersets, $J$'s, of size $j$,
%then there are
%${|I| - |K| \choose j - |K|}$ ways
%of including components other than those in $K$ into $J$,
%and the total contribution is
%$(-1)^{|I|-j} {|I| - |K| \choose j - |K|}$.
%%
%Summing over the possible sizes yields
%$$
%\sum_{j=|K|}^{|I|} (-1)^{|I|-j} {|I| - |K| \choose j - |K|}
%= (1 - 1)^{|I| - |K|} = \delta_{|I|, |K|}.
%$$
%This shows that the total contribution of $\delta W_K$
%to the right-hand side of Eq. \eqref{eq:dWI_WJ}
%vanishes unless $K = I$,
%which is the intended result.

\bibliography{simul}
\end{document}

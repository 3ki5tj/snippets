\documentclass[reprint]{revtex4-1}
\usepackage{amsmath}
\begin{document}


\section{Background}



\subsection{Flat-distribution sampling}



Consider the problem of sampling a system
using the method of Markov chain Monte Carlo (MC).
%
We are interested in the distribution, $p^*(z)$,
along a quantity $z$.
%
In practice, we shall discretize $z$
such that each integer $i$ represents
a small interval, or a bin, $(z, z + dz)$.
%
In this way,
the distribution can be written as $p^*_i$,
and the potential of mean force (PMF)
is given by
%
\begin{equation}
\phi^*_i = -\log p^*_i.
\label{eq:pmf_def}
\end{equation}
%
An example of the discrete variable $i$
is the energy $E$ of the Ising model
in the canonical ensemble,
in which $p^*_E \propto g_E \, e^{-\beta \, E}$
with $g_E$ and $\beta$ being the density of states,
and reciprocal temperature, respectively.



For a large system,
the distribution $p^*_i$ is often
narrowly peaked at a certain $i_0$.
%
Thus to find out the global property,
it is often desirable to carry out
a biased sampling that targets
a wider distribution $p_i$.
%
%Here, we refer to simulations that target
%a flat or nearly flat distribution
%as entropic or multicanonical sampling.



To do so, we need to introduce a bias potential $\phi_i$,
such that the target distribution is changed to
%
\begin{equation}
  \pi_i \propto p^*_i \, e^{-\phi_i}.
  \label{eq:pi_p_phi1}
\end{equation}
%
Upon normalization $\sum_{i = 1}^n \pi_i = 1$,
we get
%
\begin{equation}
  \pi_i =
  \frac{ p^*_i \, e^{-\phi_i} }
  { \sum_{j = 1}^n p^*_j \, e^{-\phi_j} }.
  \label{eq:pi_p_phi}
\end{equation}



From Eq. \eqref{eq:pi_p_phi1},
it is clear that to achieve the target distribution
$\pi_i = p_i$ requires
%
\begin{equation}
\phi_i = \ln p^*_i - \ln p_i + c
=\phi^*_i - \ln p_i + c
\label{eq:bias_pmf}
\end{equation}
Particularly,
to achieve a flat distribution $p_i$,
the bias potential $\phi_i$
must coincide with the PMF $\phi^*_i$,
up to an additive constant.
%
Since the target distribution
can be estimated from the histogram
accumulated from the trajectory,
Eq. \eqref{eq:pi_p_phi1}
allows us to adjust the bias potential
inversely according to the observed histogram.
%
Once the observed histogram matches
the target distribution, $p_i$,
the PMF, $\phi^*_i$,
can be deduced from Eq. \eqref{eq:bias_pmf}.

The above inversion process can be difficult
in practice,
for it depends on the precision of the histogram,
which requires some estimation
of the autocorrelation time.



\subsection{Wang-Landau algorithm}



The Wang-Landau (WL) algorithm is a convenient technique
of guessing the optimal bias potential $\phi_i$
on the fly.
%
To simplify the notation, we introduce
a shifted bias potential
%
\begin{equation}
  v_i \equiv \phi_i - \ln p^*_i + \ln p_i.
  \label{eq:vi_def}
\end{equation}
%
%
and in $v_i$, Eq. \eqref{eq:pi_p_phi}
becomes
%
\begin{equation}
  \pi_i = \frac{ p_i e^{-v_i} }
  { \sum_{j = 1}^n p_j e^{-v_j} }
  \propto p_i \, e^{-v_i}.
  \label{eq:pi_p_v}
\end{equation}
%
According to Eq. \eqref{eq:bias_pmf},
$v_i$ should approach a constant of $i$
upon convergence,



In the WL algorithm, $v_i$ are updated
in each MC step $t$,
and they may be written as $v_i(t)$.
%
The updating formula is
%
\begin{equation}
  v_i(t+1)
  =
  v_i(t)
  +
  \delta_{i, \, i(t)}
  \frac{ \alpha(t) } { p_i }.
  \label{eq:wl_update}
\end{equation}
%
where $i(t)$ is the bin at step $t$,
and $\alpha(t)$ is the updating magnitude.
%
The values of $v_j(t)$ at $j \ne i(t)$
are kept unchanged.
%
One can show that with a constant $\alpha(t) = \alpha > 0$,
the distribution collected from
the trajectory is identical to $p^*_i$.
%
However, it turns out that
the updating scheme brings about an error,
which means that $v_i(t)$ will not be a constant.



The source of the deviation is two-fold.
%
First, since $v_i(t)$ is updated continuously,
there is a random noise that is proportional
to $\sqrt \alpha$.
%
Second, there is a systematic error
that comes from the updating dynamics itself,
as it breaks the Markovian nature
that underlies Eq. \eqref{eq:pi_p_v}.
%
Thus, one has to decrease $\alpha$ over time
in order to reduce the error of $v_i$.



In the original WL scheme,
the updating magnitude $\alpha$ (or $\ln f$
in the original paper) is kept as a constant
for a period of time,
which is referred to as a stage below.
%
The histogram collected in the stage is monitored.
%
Once the histogram is sufficiently flat,
we are allowed to enter a new stage
with a reduced $\alpha$
(usually half as large as
that in the previous stage).
%
This scheme works well for early stages.
%
However, in later stages, it tends to reduce $\alpha$
too quickly, making the asymptotic error
saturate.



\subsection{$1/t$ formula}



A more effective way
of updating the $\alpha(t)$
is to discard the stage-wise setup,
and to follow the formula
%
\begin{equation}
  \alpha(t) = \frac{1}{t},
  \label{eq:alpha_invt}
\end{equation}
%
where $t$ is the number of steps
from the beginning of the MC simulation,
which shall be referred to as the ``time'' below.
%
This surprisingly simple formula has attracted
several studies, notably the one by Zhou.
%
There are, however, some reservations about
the constant of proportionality:
the optimal formula for $\alpha(t)$
might be $C/t$ with a constant $C$
different from $1.0$.



Here we shall attempt to derive
the optimal $\alpha(t)$ for a class of
multiple-bin updating schemes
that generalizes Eq. \eqref{eq:wl_update}.
%
In the multiple-bin case,
the updates of each step affect
not only the current bin $i(t)$,
but also a few neighboring ones
(such schemes are designed
to maintain a smooth bias potential).
%
We shall show that
the coefficient of proportionality, $1.0$
in Eq. \eqref{eq:alpha_invt}
requires modifications in multiple-bin update scheme,
but not in the single-bin updating scheme,
Eq. \eqref{eq:wl_update}
(as in the WL algorithm).
%
Below, we shall first derive the optimal $\alpha(t)$
for the single-bin updating scheme
in Section \ref{sec:single-bin},
and handle the multiple-bin case
in Section \ref{sec:multiple-bin}.



\section{\label{sec:single-bin}
Single-bin update}



In this section,
we shall derive the optimal $\alpha(t)$
for the single-bin updating scheme,
Eq. \eqref{eq:wl_update}.
%
To do so,
we shall first express the error of $v(t)$
as a functional of $\alpha(t)$,
and then find the optimal value
by functional variation.



\subsection{Differential equation}



Our first step is to approximate Eq. \eqref{eq:wl_update}
by a differential equation
%
\begin{equation}
  \dot v_i(t)
  =
  h_i(t) \frac{ \alpha(t) } { p_i },
  \label{eq:vt_diffeq}
\end{equation}
%
where
$\dot v_i = dv_i/dt$,
%
and $h_i(t) = \delta_{i, i(t)}$
is the instantaneous histogram,
which is equal to $1.0$
for the current bin $i(t)$
or zero otherwise.



Next, we split $h_i(t)$ into a deterministic part
and a noise
%
\begin{equation}
  h_i(t) = \langle h_i(t) \rangle + \zeta_i(t).
  \label{eq:h_split}
\end{equation}
%
Here, the deterministic part is defined
as the ``ensemble average'' of $h_i$.
%
The ensemble consists of many similar simulation copies
that share the same protocol $\alpha(t)$
and have reached the same $v_i(t)$
at time $t$.
%
The initial states and random numbers of the copies
may, however, be different.



For sufficiently small $\alpha(t)$,
the sampling process is approximately
Markovian of a finite order,
and we may assume Eq. \eqref{eq:pi_p_v}
for the deterministic part
%
\begin{equation}
  \langle h_i(t) \rangle
  \approx
  \frac{ p_i \, e^{-v_i} }
  { \sum_{j = 1}^n p_j \, e^{-v_j} }.
  \label{eq:h_ave}
\end{equation}



For the noise part, we have
%
\begin{equation}
  \langle \zeta_i(t) \rangle = 0.
  \label{eq:zeta_zeromean}
\end{equation}
%
The noise is not necessarily white.
However, we require that the correlation
function depends only on the time difference:
%
\begin{equation}
  \langle \zeta_i(t) \, \zeta_j(t') \rangle
  =
  \sigma_{ij}(t - t'),
  \label{eq:zeta_zeta_correlation}
\end{equation}
%
with $\sigma_{ij}(t)$ being an even function of $t$.



\subsection{Linear approximation}



To proceed, we first observe that
the average $\bar v = \sum_{i = 1}^n p_i \, v_i$
increases steadily over time:
%
\begin{equation}
\frac{ d \bar v } { d t }
=
\sum_{i = 1}^n p_i \dot v_i
=
\alpha(t) \sum_{i = 1}^n h_i(t) = \alpha(t).
\label{eq:dvbardt}
\end{equation}
%
However, the difference between $v_i$ and $\bar v$,
%
\begin{equation}
  x_i \equiv v_i - \bar v = v_i - \sum_{j = 1}^n p_j \, v_j,
  \label{eq:x_def}
\end{equation}
%
is expected to be small in the asymptotic regime,
and we can expand Eq. \eqref{eq:h_ave} as
$$
\begin{aligned}
\langle h_i(t) \rangle
&\approx
\frac{ p_i \, e^{- x_i} }
{ \sum_{ j = 1}^n p_j \, e^{- x_j} }
\approx
\frac{ p_i ( 1 - x_i ) }
{ \sum_{ j = 1}^n p_j (1 - x_j) }
\\
&\approx
p_i \, \left(
  1 - x_i + \sum_{j=1}^n p_j \, x_j
\right),
\end{aligned}
$$
where we have used $\sum_{j=1}^n p_j = 1$.
%
Using this in Eqs. \eqref{eq:vt_diffeq}
and \eqref{eq:h_split} yields
a set of decoupled equations
%
\begin{equation}
  \dot x_i(t)
  =
  -\alpha(t) \, \left[ x_i(t) - \xi_i(t) \right].
  \label{eq:dxdt_WL}
\end{equation}
%
where
%
\begin{equation}
  \xi_i \equiv \frac{ \zeta_i } { p_i },
  \label{eq:xi_def}
\end{equation}
and we have used Eq. \eqref{eq:dvbardt}.

The error of the bias potentials can be written in $x_i$
\begin{equation}
\sum_{i = 1}^n p_i \, \langle x_i^2 \rangle
=
\sum_{i = 1}^n p_i \, \left\langle (v_i - \bar v)^2 \right\rangle,
\label{eq:error_sum}
\end{equation}
where
$\bar v = \sum_{i = 1}^n p_i v_i$.
%
Our aim is to find the $\alpha(t)$
that minimizes Eq. \eqref{eq:error_sum}.
%
To do so, we shall first consider
a simpler one-variable problem.



\subsection{One-variable problem}



Consider the following equation
of a single variable:
%
\begin{equation}
\dot x(t) = -\alpha(t) \, \lambda \left[ x(t) - \xi(t) \right],
\label{eq:dxdt_alpha}
\end{equation}
%
where $\xi(t)$ is a generalized noise
that is invariant under time translation:
%
\begin{equation}
\left\langle \xi(t) \, \xi(t') \right\rangle
=
\kappa(t - t').
\label{eq:noise_correlation}
\end{equation}
%
Additionally, we require
\begin{equation}
  \lim_{t \rightarrow \pm\infty} \kappa(t) = 0.
  \label{eq:kappat_limit}
\end{equation}
%
For example, for a white noise,
$\kappa(t)$ is proportional to
Dirac's $\delta$-function, $\delta(t)$.
%
We wish to show that the optimal $\alpha(t)$
of minimizing $\langle x^2(t) \rangle$ at long times
is given by
%
\begin{equation}
  \alpha(t) = \frac{1}{\lambda \, (t + t_0)}.
\label{eq:alpha_opt}
\end{equation}



To do so, we first recall
the formal solution of Eq. \eqref{eq:dxdt_alpha}:
%
\begin{equation}
x(t) = x(0) \, e^{-\lambda \, q(t)}
+ \int_0^t \dot u(t') \, \xi(t') \, dt',
\label{eq:xt_solution}
\end{equation}
%
where,
%
\begin{equation}
q(t) \equiv \int_0^t \alpha(t') \, dt',
\label{eq:qt_definition}
\end{equation}
%
and
%
\begin{align}
u(q(t'))
&\equiv
e^{-\lambda \, q(t) + \lambda \, q(t')}.
\label{eq:ut_definition}
\end{align}


We shall further demand that
%
\begin{equation}
  \lim_{t \to \infty} q(t) \to \infty.
  \label{eq:qt_limit}
\end{equation}
%
Then, for a large time $t$,
the first term on the right-hand side
of Eq. \eqref{eq:xt_solution} can be neglected, and
%
\begin{align}
\left\langle x^2(t) \right\rangle
%&=
%\int_0^t \int_0^t \dot u(t') \, \dot u(t'')
%    \left\langle \xi(t') \xi(t'') \right\rangle dt'' \, dt'
%\notag
%\\
&=
\int_0^t \int_0^t
  \dot u(q(t')) \, \dot u(q(t'')) \,
  \kappa(t' - t'') \, dt'' \, dt'.
\label{eq:x2t_average}
\end{align}



Variating this expression and
using the Euler-Lagrange equation yields
$$
\begin{aligned}
0
&=
\frac{d}{d\tau} \int_0^t \dot u(q(t')) \, \kappa(t' - \tau) \, dt'
= \int_0^t \ddot u(q(t')) \, \kappa(t' - \tau) \, dt',
\end{aligned}
$$
where we have dropped the boundary terms
by using Eq. \eqref{eq:kappat_limit}.
%
%In fact, we can repeat the process $n$ times, and
%$$
%\int_0^t u^{(n)}(t') \, \kappa(t' - \tau) \, dt' = 0,
%\qquad (n \ge 1)
%$$
%
In order for this to hold for any $\tau$,
we must have
%
\begin{equation}
\ddot u(q(t')) = 0,
\qquad
\mathrm{or}
\;\;
\dot u(q(t')) = c,
\label{eq:ddu_eq_0}
\end{equation}
%
where $c$ is a constant of $t'$.
%
Using Eq. \eqref{eq:ut_definition}
we get
$$
e^{-\lambda \, q(t) + \lambda \, q(t')}
=
c \, (t' + t_0),
$$
where $t_0$ is a constant.
%
Taking the logarithm, and differentiating this with respect to $t'$
yields Eq. \eqref{eq:alpha_opt}.



\subsection{Optimal $\alpha(t)$ for the single-bin update}



Let us consider minimizing Eq. \eqref{eq:error_sum}
for the single-bin updating scheme.
%
From \eqref{eq:dxdt_WL}, we find that
with $\lambda = 1$,
Eq. \eqref{eq:alpha_opt}
will optimize each term $\langle x_i^2 \rangle$,
and hence the sum.
%
Thus, Eq. \eqref{eq:alpha_invt} is optimal
up to a constant defining the origin of $t$.



\section{\label{sec:multiple-bin}
Multiple-bin update}



We can readily extend the above analysis to
the multiple-bin updating scheme.
%
In this case,
upon a visit to bin $j$,
we update not only $v_j$,
but also $v_i$ for a few neighboring bins $i$.
%
The update magnitudes are given by a matrix $\mathbf w$
with elements $w_{ij}$.
%
Then, Eq. \eqref{eq:vt_diffeq} is generalized to
\begin{equation}
  \dot v_i(t) =
  \sum_{j=1}^n \alpha(t) \, h_j(t) \frac{ w_{ij} } { p_j }.
  \label{eq:vt_diffeq_mbin}
\end{equation}


The procedure of optimization is largely
the same as the single-bin case,
whereas the matrix nature brings about
some difficulties that require
certain limitations and approximations.



\subsection{Updating matrix $\mathbf w$}



The matrix $\mathbf w$ is not arbitrary.
%
A necessary condition to sample desired distribution
$\mathbf p = (p_1, \dots, p_n)$
is that in Eq. \eqref{eq:vt_diffeq_mbin}
when $h_j(t)$ coincides with $p_j$,
the rate of change $\dot v_i(t)$
should be independent of $i$.
%
This allows $v_i(t)$ to approach a constant
asymptotically.
%
Thus, by a proper scaling of $\alpha(t)$,
we may write this condition as
%
\begin{equation}
  \sum_{j = 1}^n w_{ij} = 1.
  \label{eq:w_sumj}
\end{equation}
%
In other words, $(1, \dots, 1)^T$
is a right eigenvector of $\mathbf w$
with eigenvalue $1$.
%
We shall further assume this eigenvalue
is the largest of all eigenvalues.
%
In this way,
the transpose $\mathbf w^T$
resembles a transition matrix.



To simplify the following discussion,
we shall further impose a stronger
detailed balance condition:
%
\begin{equation}
  p_i \, w_{ij} = p_j \, w_{ji}.
  \label{eq:w_detailedbalance}
\end{equation}
%
It follows that
\begin{equation}
  \sum_{i = 1}^n p_i \, w_{ij}
  =
  \sum_{i = 1}^n p_j \, w_{ji}
  = p_j,
  \label{eq:w_balance}
\end{equation}
i.e., $\mathbf p$ is a left eigenvector of $\mathbf w$.


Further, by Eq. \eqref{eq:w_detailedbalance},
we can define a symmetric matrix $\hat{\mathbf w}$
as $\hat w_{ij} = \sqrt{p_i/p_j} \, w_{ij}$
that can be diagonalized
with a set of orthonormal eigenvectors:
%
$\sum_{i = 1}^n U_{ki} \, \hat w_{ij} = \lambda_k \, U_{kj}$.
%
Thus,
in terms of diagonalizing $\mathbf w$, we have
\begin{equation}
  \sum_{i = 1}^n T_{ki} \, w_{ij} = \lambda_k \, T_{kj},
  \label{eq:T_w}
\end{equation}
where
$T_{ij} = \sqrt{p_j} \, U_{ij}$ satisfies
the orthonormal conditions:
%
\begin{align}
\sum_{k = 1}^n T_{ki} \, T_{kj}
&= \delta_{ij} \, p_i,
\label{eq:T_orthonormal_cols}
\\
\sum_{k = 1}^n \frac{ T_{ik} \, T_{jk} }{ p_k }
&= \delta_{ij}.
\label{eq:T_orthonormal_rows}
\end{align}



\subsection{Linearization and decoupling}



We can now simplify Eq. \eqref{eq:vt_diffeq_mbin}.
%
By using Eq. \eqref{eq:w_balance},
we can show that \eqref{eq:dvbardt} remains true.
%
Thus, for Eq. \eqref{eq:x_def}, we have
%
\begin{equation}
  \sum_{i=1}^n p_i \, x_i = 0.
\end{equation}
%
and
%
$$
\begin{aligned}
\dot x_i
&= \alpha(t) \sum_{j=1}^n w_{ij}
\left( \frac{ h_j } { p_j }  - 1 \right)
\\
&=
-\alpha(t) \sum_{j = 1}^n
w_{ij} \left[ x_j(t) - \frac{\zeta_j (t)}{p_j} \right],
\end{aligned}
$$
where
we have expanded the right-hand side
in linear terms of $x_j$ in the second step.


Now by diagonalizing the matrix $\mathbf w$
using matrix defined in Eqs. \eqref{eq:T_w},
\eqref{eq:T_orthonormal_cols},
and
\eqref{eq:T_orthonormal_rows},
we get a set of decoupled equations
%
\begin{equation}
\dot y_i(t)
=
-\alpha(t) \, \lambda_i
[y_i(t) - \eta_i(t)],
\label{eq:yt_diffeq}
\end{equation}
%
where
\begin{align}
  y_i &= \sum_{j=1}^n T_{ij} \, x_j,
  \label{eq:y_def}
  \\
  \eta_i &= \sum_{j=1}^n T_{ij} \frac{ \zeta_j}{ p_j},
  \label{eq:eta_def}
\end{align}


\subsection{Target function}


Further, we have from Eq. \eqref{eq:T_orthonormal_cols},
\begin{align}
  \mathcal E
  =
  \sum_{k = 1}^n y_k^2
  =
  \sum_{i, j, k=1}^n T_{ki} \, T_{kj} \, x_i \, x_j
  = \sum_{i = 1}^n p_i \, x_i^2.
  \label{eq:y2_sum}
\end{align}
%
If we ordered $y_k$ in descending order of $\lambda_k$,
%
then, the mode $y_1 = \sum_{j=1}^n p_j \, x_j$
corresponds to $\lambda_1 = 1$,
vanishes.
%
Thus, the sum of $y_k$ in \eqref{eq:y2_sum}
can start from $i = 2$.



In analogous to Eq. \eqref{eq:x2t_average},
the error function defined in Eq. \eqref{eq:y2_sum}
can be written as
%
\begin{align}
  S
  &= \sum_{i = 2}^n \langle y_i^2 \rangle
  \notag
  \\
  &=
  \int_0^t \int_0^t
  \sum_{i = 2}^n
  \dot u_i(q(t')) \,
  \dot u_i(q(t'')) \,
  \kappa_i(t' - t'') \, dt' \, dt'',
  \label{eq:y2_int}
\end{align}
%
where
\begin{align}
  u_i(q(t'))
  &\equiv
  e^{\lambda_i \, [q(t') - \, q(t)]}
  \\
  \kappa_i(t - t')
  &=
  \left\langle \eta_i(t) \, \eta_i(t') \right\rangle.
\end{align}
%
The extremal condition of $S$ is
%
\begin{align}
\sum_{i=2}^n
u_i'(q(\tau))
\int_0^t
\ddot u_i(q(t')) \, \kappa_i(t' - \tau) \, dt' = 0.
\label{eq:optimal_mbin1}
\end{align}
%
If all $\lambda_i$ are the same,
so are all $u_i$,
and the above condition can be satisfied
by Eq. \eqref{eq:ddu_eq_0},
which is the single-bin case.



\subsection{Optimal $\alpha(t)$}


The general solution of Eq. \eqref{eq:optimal_mbin1}
is harder to obtain.
%
To proceed, we shall further assume perfect sampling,
%
\begin{equation}
  \langle \zeta_i(t) \, \zeta_j(t') \rangle
  =
  p_i \, \delta_{ij} \, \delta(t - t').
  \label{eq:zz_perfect}
\end{equation}
%
Thus, from Eqs. \eqref{eq:T_orthonormal_rows} and \eqref{eq:eta_def},
we get
%
\begin{equation}
  \kappa_i(t - t')
  =
  \sum_{j,k = 1}^n
  \frac{ T_{ij} } { p_j }
  \frac{ T_{ik} } { p_k }
  \langle \zeta_j(t) \, \zeta_k(t') \rangle
  =
  \delta(t - t').
  \label{eq:kappa_perfect}
\end{equation}
%
Thus, Eq. \eqref{eq:optimal_mbin1}
is simplified as
%
\begin{align}
  \sum_{i=1}^n
  \ddot u_i(q(\tau)) \, u'_i(q(\tau)) = 0.
  \label{eq:optimal_mbin2}
\end{align}
%
We can study two limiting cases.
%
For $\tau \ll t$,
the dominant contribution comes from
the term(s) with the smallest eigenvalue $\lambda_i = \lambda_n$,
which leads to
$$
\ddot u_n(\tau) = 0,
\quad
\dot u_n(\tau) = c,
\quad
u_n(\tau) = c \, (\tau + t_0),
$$
which means
$$
\alpha(\tau) \approx \frac{1}{\lambda_n \, (\tau + t_0)}.
$$


A more direct, and often more accurate, approach
is to assume the functional form
$$
\alpha(\tau) = \frac{1}{\lambda \, (\tau + t_0) }.
$$
Using this and Eq. \eqref{eq:kappa_perfect}
in Eq. \eqref{eq:y2_sum}, we get
%
\begin{equation}
S
=
\int_0^t
\sum_{i=0}^n \dot u_i^2(q(t')) \, dt'
\stackrel{t \to \infty}
{=\joinrel=\joinrel=}
\frac{1}{t+t_0}
\sum_{i = 2}^n
\frac{ \lambda_i^2 } { \lambda \, (2 \, \lambda_i - \lambda) },
\label{eq:y2_sum_perfect_invt}
\end{equation}
%
and the optimal $\lambda$
can be found from the smallest root of
%
\begin{equation}
\sum_{i = 2}^n
\frac{ \lambda_i - \lambda }
{ \left(2 - \lambda/ \lambda_i \right)^2 }
= 0.
\end{equation}



Under the approximation, we can make an interesting observation
on the optimal updating scheme.
%
Since
$$
\frac{ \lambda_i^2 }{ \lambda \, (2 \lambda_i - \lambda) } \ge 1,
$$
with the equality achieved only at $\lambda = \lambda_i$,
we see the from Eq. \eqref{eq:y2_sum_perfect_invt}
that the minimal error is achieved with
\begin{equation}
  \lambda_2 = \dots = \lambda_n = \lambda.
\end{equation}
%
An updating matrix $\mathbf w$ satisfying this condition
can be written as
$$
\mathbf w
=
\lambda \,
\left(
\begin{array}{ccccc}
  1 & \dots & 0 \\
  \vdots &\ddots & \vdots \\
  0 & \dots & 1
\end{array}
\right)
+
c' \,
\left(
\begin{array}{ccccc}
  p_1 & \dots & p_n \\
  \vdots & & \vdots \\
  p_1 & \dots & p_n
\end{array}
\right).
$$
%
This shows that in terms of convergence,
the single-bin updating scheme
(as adopted in the WL algorithm)
is one of the most efficient candidates
of all multiple-bin schemes.



\end{document}
